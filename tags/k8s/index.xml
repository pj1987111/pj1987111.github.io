<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on ZHY ZONE</title>
    <link>https://pj1987111.github.io/tags/k8s/</link>
    <description>Recent content in k8s on ZHY ZONE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Jul 2019 12:04:05 +0800</lastBuildDate>
    
	<atom:link href="https://pj1987111.github.io/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Nginx&#43;Ingress-controller解决L7外网Web服务暴露和负载均衡</title>
      <link>https://pj1987111.github.io/posts/k8s/nginx&#43;ingress-controller%E8%A7%A3%E5%86%B3l7%E5%A4%96%E7%BD%91web%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Sun, 28 Jul 2019 12:04:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/nginx&#43;ingress-controller%E8%A7%A3%E5%86%B3l7%E5%A4%96%E7%BD%91web%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>k8s小技巧</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%B0%8F%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Fri, 26 Jul 2019 10:44:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%B0%8F%E6%8A%80%E5%B7%A7/</guid>
      <description> 1 设置默认ns 1.1 问题描述： 之前装过jhub，后来删除，jhub创建时候会修改namespace从default到jhub。后来导致所有缺省namespace的应用都装在jhub上，手动删除jhub的ns后新pod创建不上，报错找不到jhub。
1.2 解决方式： 输入kubectl config view命令查看与context相关的配置，如下图所示，可见name是kubernetes-admin@kubernetes的namespace是jhub。 输入kubectl config set-context $(kubectl config current-context) &amp;ndash;namespace=default 命令将namespace的jhub改成default即可。 $(kubectl config current-context)的输出是kubernetes-admin@kubernetes。修改后输出结果如下图所示。 </description>
    </item>
    
    <item>
      <title>k8s部署dashboard与踩坑</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2dashboard%E4%B8%8E%E8%B8%A9%E5%9D%91/</link>
      <pubDate>Tue, 23 Jul 2019 13:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2dashboard%E4%B8%8E%E8%B8%A9%E5%9D%91/</guid>
      <description>1 简介 使用kube-dashboard来完成日常的一些k8s集群运维工作。
Dashboard是Kubernetes社区官方开发的仪表板，有了仪表板后管理者就能够透过 Web-based 方式来管理 Kubernetes 集群，除了提升管理方便，也让资源可视化，让人更直觉看见系统信息的呈现结果。
下面我们就来看如何基于部署kube-dashboard。
2 安装kube-dashboard组件 在安装之前，先通过删除namespace来清除之前版本的内容，通过命令
kubectl delete ns kubernetes-dashboard  下载yaml文件，使用目前最新的v2.0.0-beta2版本，很好的适应k8s-1.15.0版本。 需要修改两处地方： * 修改镜像地址 sed命令中第一个匹配 * 默认配置文件中ClusterRole为kubernetes-dashboard权限过低，修改成cluster-admin sed命令中第二个匹配，上一行满足替换下一行的模式。 为何更改，下文将详细介绍。
#官网下载yaml curl -o dashboard-v2.0.0-beta2.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta2/aio/deploy/recommended.yaml #更改镜像地址，改成私仓的地址 sed -i &amp;quot;s/kubernetesui/10.57.26.15:4000/g;/kind: ClusterRole/{n;s/name: kubernetes-dashboard/name: cluster-admin/g}&amp;quot; dashboard-v2.0.0-beta2.yaml #安装 kubectl create -f dashboard-v2.0.0-beta2.yaml  然后可以通过查看安装组件，以及部署状态。 3 dashboard的注意点 测试使用的k8s是1.15.0版本，目前适应的dashboard是dashboard:v2.0.0-beta2版本。 新建的dashboard在kubernetes-dashboard位置
3.1 dashboard访问 这是我认为dashboard部署的重点，也是坑比较多的地方，我在里面陷了很久，在此分享一下。 我尝试了4种访问dashboard服务的方式，其他服务都可以借鉴。
3.1.1 kubectl proxy kubectl proxy 的原理是将机器与kubernetes API Server之间做一个代理，默认情况下，只能从本机访问。 可以使用kubectl cluster-info命令检查配置是否正确，集群是否可以访问。 启动代理只需执行如下命令即可：
$ kubectl proxy Starting to serve on 127.</description>
    </item>
    
    <item>
      <title>k8s高可用架构</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:02 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/</guid>
      <description>1 与keepalive+VIP对比 网上大量的人使用KeepAlive+VIP的形式完成高可用，这个方式有两个不好的地方：
其一，受限于使用者的网络，无法适用于SDN网络，比如Aliyun的VPC。
其二，虽然是高可用的，但是流量还是单点的，所有node的的网络I/O都会高度集中于一台机器上（VIP)，一旦集群节点增多，pod增多，单机的网络I/O迟早是网络隐患。
2 两种模式 2.1 堆叠式etcd拓扑 堆叠式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在同一台服务器上，共同被kubeadm所管理。
每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。
每个控制平面节点都创建一个本地etcd，这个本地etcd只与该控制平面节点的kube-apiserver通讯，kube-scheduler和kube-controller-manager也是节点独立的。
介绍一下这种方案的优缺点： 优点：这种控制平面和etcd部署在同一个节点的模式比控制平面和etcd分离部署安装容易并且易于管理副本。 缺点：这种模式存在错误耦合的缺点，当一个节点宕机，etcd和控制平面实例都丢失，只能添加更多的控制平面节点来缓和这个问题。
在高可用集群下需要运行至少3个堆叠控制平面节点。
这是kubeadm高可用的默认实现方案，运行kubeadm init和kubeadm join &amp;ndash;control-plane后本地etcd将被自动创建。 2.2 分离式etcd拓扑 分离式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在不同服务器上。
类似于堆叠式架构，每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。不同的地方在于，etcd在不同的host上运行，每个etcd都会和每个控制平面上的kube-apiserver交互。
这种拓扑结构将etcd与控制平面进行了分离。这种方式与堆叠式架构相比可以在etcd的节点丢失以及控制平面的节点丢失上有更高的容错性。
然而，这种拓扑结构在机器数量上对比堆叠式架构需要两倍的数量。需要至少3个etcd节点以及3个控制平面节点。 3 安装与部署 在接下来的介绍文章中会对这两种模式的安装部署方式做详细介绍，请看</description>
    </item>
    
    <item>
      <title>kubeadm安装高可用k8s集群</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:00 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>1 前置安装 1.1 环境相关 安装docker和kubeadm，在每一台机器上都需要安装。
1:ssh可信 2:hosts cat&amp;gt;&amp;gt;/etc/hosts&amp;lt;&amp;lt;EOF 10.57.26.15 k8s-01 10.57.26.7 k8s-02 10.57.26.8 k8s-03 10.57.26.26 k8s-04 10.57.26.11 k8s-05 10.57.26.17 k8s-06 10.57.26.33 k8s-07 EOF host名需要和 hostname /etc/hostname 一致 3:安全相关 setenforce 0 sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config systemctl disable firewalld.service systemctl stop firewalld.service systemctl disable iptables.service systemctl stop iptables.service 4:源和代理 echo proxy=http://10.57.22.8:3128/ &amp;gt;&amp;gt; /etc/yum.conf cat&amp;gt;/etc/yum.repos.d/docker-ce.repo&amp;lt;&amp;lt;EOF [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg EOF cat&amp;gt;/etc/yum.repos.d/kubernetes.repo&amp;lt;&amp;lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.</description>
    </item>
    
    <item>
      <title>Kubeadm离线安装单master节点k8s</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:11 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</guid>
      <description>1 目标 1.在所有节点上安装docker和kubeadm
2.部署kubernetes master
3.部署容器网络插件
4.部署kubernetes worker
5.部署dashboard可视化插件
6.部署容器存储插件
2 准备(所有安装机器均需执行) 2.1:/etc/hosts中添加ip和host的映射 2.2:master和node做互信 2.3:关闭防火墙和selinux 1) 关闭selinux
sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config  2) 关闭防火墙
systemctl disable firewalld.service systemctl stop firewalld.service  3) 关闭iptables
systemctl disable iptables.service systemctl stop iptables.service  2.4:安装docker 将离线包解压，进docker目录，运行sudo yum install * 安装好后再执行
systemctl start docker systemctl enable docker  将docker服务启动
2.5:私仓搭建 这里选择了docker的registry2来建立私有仓库。
 下载镜像仓库
docker pull registry:2  启动镜像仓库
docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 --restart=always --name registry registry:2  配置阿里云的Docker加速器，加快pull registry镜像</description>
    </item>
    
    <item>
      <title>K8s安装离线依赖库与镜像制作</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</guid>
      <description>一：docker环境下载 1.1 yum仓库添加 vim /etc/yum.repos.d/docker-ce.repo 添加以下内容： [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg  推荐添加阿里云的源，速度非常快。
1.2 docker环境本地下载 yum install --downloadonly --downloaddir=. docker-ce-18.09.4-3.el7  该命令会把docker以及依赖全下载到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
二：kubeadm环境下载 2.1 yum仓库添加 vim /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg  2.2 kubeadm环境本地下载 yum install --downloadonly --downloaddir=. kubeadm  该命令会把kubeadm以及依赖全下载包括kubelet,kubectl到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
三：k8s依赖镜像下载 kubeadm安装时会访问特定的tag，若不存在就会走外网下载，所以kubeadm部署前必须将所需的k8s环境以镜像方式下载好，并导入docker中
3.1 阿里云海外镜像下载配置 由于各种原因，k8s的依赖kube-proxy，kube-apiserver，kube-controller-manager，kube-scheduler，coredns，etcd和pause都难于下载，本人通过阿里云海外镜像下载方式，分享一下。 1) 将github上建立一个项目，github的配置不再赘述。 2) 将所有依赖建立目录，并在每个目录下建立Dockerfile，并输入FROM命令，每个组件的命令如下：
FROM k8s.gcr.io/kube-apiserver:v1.14.2 FROM k8s.gcr.io/kube-proxy:v1.14.2 FROM k8s.gcr.io/kube-controller-manager:v1.14.2 FROM k8s.gcr.io/kube-scheduler:v1.14.2 FROM k8s.</description>
    </item>
    
    <item>
      <title>harbor仓库搭建手册</title>
      <link>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</link>
      <pubDate>Fri, 21 Jun 2019 23:21:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</guid>
      <description>一 harbor介绍 二 环境准备 centos 7.6.1810 docker 18.09.4 docker-compose 1.21.2
2.1 安装docker 2.2 安装docker-compose 下载并解压后，将docker-compose赋权限并放置环境变量下。
三 搭建harbor 3.1 下载离线包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz  下载并解压
3.2 修改harbor配置(http) harbor ui端口默认是80端口，会和大多数应用冲突，所以需要更改，要改两个地方。第一个地方是docker-compose.yml文件，将ports 80:80改成 xxx:80即可，如下图所示： 第二个地方是harbor.yml文件，将http:port: 80更改成需要的，并将hostname改成ip或域名，这个hostname需要与pull和push的前缀一致。如图所示 3.3 生成相关配置并安装 运行 ./prepare以及./install.sh
3.4 启动 docker-compose up -d  3.5 启动后相关容器 正常启动后会有9个容器，如下图所示，其中nginx-photon:v1.8.0容器可以看到端口映射4001-&amp;gt;80 四、客户端docker配置 vi /etc/docker/daemon.json 将&amp;quot;10.57.30.22:4001&amp;quot;添加insecure-registries中 systemctl daemon-reload systemctl restart docker  五、访问 Web UI 并测试 5.1、主页 访问主页http://10.57.30.22:4001 即可看到登陆页面，输入密码后进入，默认账户是admin，密码在harbor.yml中的harbor_admin_password字段。 进入后如下显示： 5.2 创建项目 Harbor 有一个项目的概念，项目名可以理解为 Docker Hub 的用户名，其下可以后很多 images，Harbor 的项目必须登录后方可 push，公有项目和私有项目的区别是对其他用户是否可见 创建项目后，才可以push和pull。</description>
    </item>
    
    <item>
      <title>k8s部署jenkins以及基本使用</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 21 Jun 2019 16:52:58 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>一：Jenkins+Kubernetes方案简介 1.1、传统Jenkins部署的问题 Jenkins 是开源的一套持续集成框架，可以进行大规模的编译、测试和发布的工作，给软件开发团队带来极大的便利性。 Jenkins 的持续集成环境可以是集群化的，主要的运行模式为master-slave模式。 Jenkins 的master为Jenkins系统的控制节点，slave节点负责具体的项目编译测试等工作。 由于大公司里面需要进行编译的工程或者对象非常庞大，因此需要大量的物理节点作为slave,而且这些环节相对固定，可能很难适应其他项目的编译测试，一旦salve节点遭到破坏，需要人为的进行修复甚至重建，非常麻烦。 2.2、与Kubernetes-plugin的结合 Docker的问世，为我们提供了解决方案，使用docker作为jenkins slave节点可以解决slave节点遭到破坏后遇到的问题， 而且大量的工程不是每时每刻同时运行的因此可以在需要时吧docker拉起来进行编译测试，这样就节约了大量的物理节点， 解决了上述问题。 然而使用docker 同样需要集群，因此要用到集群管理工具， swarm 或者kubernetes。Swarm 一般用在小集群上，而且swarm和docker本身的借口完全一致， 因此这里就简单介绍单点docker节点作为slave。 Kubernetes作为大的docker 集群管理工具，当jenkins的工程数量非常大的时候可以用kubernetes, kubernetes的运维相对比docker 和swarm的门槛要高一点。 Jenkins master也可以使用docker来运行或者使用kubernetes来提供高可用的jenkins master。 Kubernetes 主要是用来进行docker 调度运行的，同样因为这一点通过kubernetes插件的配置，不需要管slave节点是否在线，因为kubernetes帮你负责创建和连接。这样就可以做到没有任务的时候没有任务slave节点在线，做到完全的按需启动slave和调度slave。
二、在K8S上构建Jenkins Server 2.1 创建PVC存储 首先需要对jenkins-server创建一个Kubernetes持久化存储卷。为了让Jenkins Server可以具有Fail Over的能力，建议将Jenkins的数据存储到远程存储或者分布式存储上。本文使用分布式存储ceph来模拟存储效果，分布式存储的部署请参考我的《rook+ceph部署分布式文件系统》，指定rook-ceph-block并分配200G空间。
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cicdpvc namespace: cicd labels: app: cicd spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 200Gi  2.2 创建jenkins-server 创建jenkins-server的yaml如下所示
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: jenkins2 namespace: cicd spec: template: metadata: labels: app: jenkins2 spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins2 containers: - name: jenkins image: 10.</description>
    </item>
    
  </channel>
</rss>