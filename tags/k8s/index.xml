<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on ZHY ZONE</title>
    <link>https://pj1987111.github.io/tags/k8s/</link>
    <description>Recent content in k8s on ZHY ZONE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2019 11:18:23 +0800</lastBuildDate>
    
	<atom:link href="https://pj1987111.github.io/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>设置RBAC规则来实现多用户权限隔离访问</title>
      <link>https://pj1987111.github.io/posts/k8s/rbac_geli/</link>
      <pubDate>Fri, 09 Aug 2019 11:18:23 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/rbac_geli/</guid>
      <description>1 AAA和BBB产品线容器的环境准备 1.1 两个产品线的权限隔离说明 假如我们公司有productline-aaa和productline-bbb两个产品线，每个产品线都有各自相应的负责人。要求每个产品线的负责人只能管理自己产品线的容器，实现效果如下： 为了方便演示，我这里建productline-aaa和productline-bbb两个不同的命名空间，并分别在每个命名空间中创建几个容器和service。
1.2 创建两个产品线的容器环境 创建productline-aaa命名空间，并在productline-aaa命名空间中创建2个productline-aaa-testweb容器和productline-aaa-testweb服务，productline-aaa同理。
编写yaml代码 aaa产线
apiVersion: v1 kind: Namespace metadata: name: productline-aaa --- apiVersion: v1 kind: ReplicationController metadata: name: productline-aaa-testweb namespace: productline-aaa labels: name: productline-aaa-testweb spec: replicas: 2 selector: name: productline-aaa-testweb template: metadata: labels: name: productline-aaa-testweb spec: containers: - name: productline-aaa-testweb image: 10.57.26.15:4000/guestbook-php-frontend env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: productline-aaa-testweb namespace: productline-aaa labels: name: productline-aaa-testweb spec: ports: - port: 80 selector: name: productline-aaa-testweb  bbb产线</description>
    </item>
    
    <item>
      <title>Nginx&#43;Ingress-controller解决服务暴露和负载均衡</title>
      <link>https://pj1987111.github.io/posts/k8s/nginx&#43;ingress-controller%E8%A7%A3%E5%86%B3l7%E5%A4%96%E7%BD%91web%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Sun, 28 Jul 2019 12:04:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/nginx&#43;ingress-controller%E8%A7%A3%E5%86%B3l7%E5%A4%96%E7%BD%91web%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>1 环境说明和准备 Ingress 使用开源的反向代理负载均衡器来实现对外暴露服务，比如 Nginx、Apache、Haproxy等。Nginx Ingress 一般有三个组件组成：
Nginx：反向代理负载均衡器
Ingress Controller ：Ingress Controller 可以理解为控制器，它通过不断的跟 Kubernetes API 交互，实时获取后端 Service、Pod 等的变化，比如新增、删除等，然后结合 Ingress 定义的规则生成配置，然后动态更新上边的 Nginx 负载均衡器，并刷新使配置生效，来达到服务自动发现的作用。
Ingress： Ingress 则是定义规则，通过它定义某个域名的请求过来之后转发到集群中指定的 Service。它可以通过 Yaml 文件定义，可以给一个或多个 Service 定义一个或多个 Ingress 规则。
以上三者有机的协调配合起来，就可以完成 Kubernetes 集群服务的暴露。
拿前后端分离的frontend和testweb两个web服务为例，大致拓扑如下： 2 安装部署 2.1 安装部署default-backend 创建Deployment，副本为1，并创建service绑定pod。配置文件如下所示：
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: default-http-backend labels: k8s-app: default-http-backend namespace: kube-system spec: replicas: 1 template: metadata: labels: k8s-app: default-http-backend spec: terminationGracePeriodSeconds: 60 containers: - name: default-http-backend # Any image is permissable as long as: # 1.</description>
    </item>
    
    <item>
      <title>docker和k8s小技巧</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%B0%8F%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Fri, 26 Jul 2019 10:44:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%B0%8F%E6%8A%80%E5%B7%A7/</guid>
      <description> 1 设置默认ns 1.1 问题描述： 之前装过jhub，后来删除，jhub创建时候会修改namespace从default到jhub。后来导致所有缺省namespace的应用都装在jhub上，手动删除jhub的ns后新pod创建不上，报错找不到jhub。
1.2 解决方式： 输入kubectl config view命令查看与context相关的配置，如下图所示，可见name是kubernetes-admin@kubernetes的namespace是jhub。 输入kubectl config set-context $(kubectl config current-context) &amp;ndash;namespace=default 命令将namespace的jhub改成default即可。 $(kubectl config current-context)的输出是kubernetes-admin@kubernetes。修改后输出结果如下图所示。 2 一些有用的docker镜像 2.1 aerospike aerospike管理器 
docker run -ti --name aerospike-asadm --rm aerospike/aerospike-tools asadm --host 10.57.30.214 --no-config-file  aerospike工具，查看sql用 
docker run -ti --name aerospike-aql --rm aerospike/aerospike-tools aql -h 10.57.30.214 --no-config-file  2.2 mssql linux版本 注：密码必须有大小写数字超过10位，不然镜像会自动退出。 例子中将镜像中1433端口映射到宿主机的1401端口，并将镜像中的/var/opt/mssql路径映射到宿主机/var/mssql路径下，设置&amp;ndash;name sql1防止镜像重建
docker run -e &#39;ACCEPT_EULA=Y&#39; -e &#39;MSSQL_SA_PASSWORD=YourStrong!Passw0rd&#39; -p 1401:1433 -v /var/mssql:/var/opt/mssql --name sql1 -d microsoft/mssql-server-linux:2017-latest  </description>
    </item>
    
    <item>
      <title>不错的参考文章</title>
      <link>https://pj1987111.github.io/posts/k8s/%E4%B8%8D%E9%94%99%E7%9A%84%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0/</link>
      <pubDate>Tue, 23 Jul 2019 21:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/%E4%B8%8D%E9%94%99%E7%9A%84%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0/</guid>
      <description>1 kubernetes-crash-course系列 0 - Kubernetes Overview
1 - Kubernetes Objects 
2 - Kubernetes Pods
3 - Kubernetes Services and Ingress
4 - Kubernetes Storage</description>
    </item>
    
    <item>
      <title>mysql</title>
      <link>https://pj1987111.github.io/posts/k8s/mysql/</link>
      <pubDate>Tue, 23 Jul 2019 20:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/mysql/</guid>
      <description> 1 进入mysql的pod，并创建数据库，表，并插入数据。 执行 kubectl delete pod mysql-6d6cb6c594-t6k4w 后，删除老的pod，deploy会新建同样的pod做恢复。
再进入新的pod中，show database，可以发现之前那个mysql pod建立的数据库，表和数据都不存在，如图所示： </description>
    </item>
    
    <item>
      <title>k8s部署dashboard与踩坑</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2dashboard%E4%B8%8E%E8%B8%A9%E5%9D%91/</link>
      <pubDate>Tue, 23 Jul 2019 13:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2dashboard%E4%B8%8E%E8%B8%A9%E5%9D%91/</guid>
      <description>1 简介 使用kube-dashboard来完成日常的一些k8s集群运维工作。
Dashboard是Kubernetes社区官方开发的仪表板，有了仪表板后管理者就能够透过 Web-based 方式来管理 Kubernetes 集群，除了提升管理方便，也让资源可视化，让人更直觉看见系统信息的呈现结果。
下面我们就来看如何基于部署kube-dashboard。
2 安装kube-dashboard组件 在安装之前，先通过删除namespace来清除之前版本的内容，通过命令
kubectl delete ns kubernetes-dashboard  下载yaml文件，使用目前最新的v2.0.0-beta2版本，很好的适应k8s-1.15.0版本。 需要修改两处地方： * 修改镜像地址 sed命令中第一个匹配 * 默认配置文件中ClusterRole为kubernetes-dashboard权限过低，修改成cluster-admin sed命令中第二个匹配，上一行满足替换下一行的模式。 为何更改，下文将详细介绍。
#官网下载yaml curl -o dashboard-v2.0.0-beta2.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta2/aio/deploy/recommended.yaml #更改镜像地址，改成私仓的地址；并将clusterrolebinding附近name: kubernetes-dashboard改成name: cluster-admin以获得管理员权限 sed -i &amp;quot;s/kubernetesui/10.57.26.15:4000/g;/kind: ClusterRole/{n;s/name: kubernetes-dashboard/name: cluster-admin/g}&amp;quot; dashboard-v2.0.0-beta2.yaml #安装 kubectl create -f dashboard-v2.0.0-beta2.yaml  然后可以通过查看安装组件，以及部署状态。 3 dashboard访问 这是我认为dashboard部署的重点，也是坑比较多的地方，我在里面陷了很久，在此分享一下。 我尝试了4种访问dashboard服务的方式，其他服务都可以借鉴。
3.1 kubectl proxy kubectl proxy 的原理是将机器与kubernetes API Server之间做一个代理，默认情况下，只能从本机访问。 可以使用kubectl cluster-info命令检查配置是否正确，集群是否可以访问。 启动代理只需执行如下命令即可：
$ kubectl proxy Starting to serve on 127.</description>
    </item>
    
    <item>
      <title>kubeadm安装高可用k8s集群</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:00 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>0 k8s高可用架构 在介绍部署方式前，先介绍一下k8s的高可用架构。
0.1 与keepalive+VIP对比 网上大量的人使用KeepAlive+VIP的形式完成高可用，这个方式有两个不好的地方：
其一，受限于使用者的网络，无法适用于SDN网络，比如Aliyun的VPC。
其二，虽然是高可用的，但是流量还是单点的，所有node的的网络I/O都会高度集中于一台机器上（VIP)，一旦集群节点增多，pod增多，单机的网络I/O迟早是网络隐患。
0.2 两种模式 0.2.1 堆叠式etcd拓扑 堆叠式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在同一台服务器上，共同被kubeadm所管理。
每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。
每个控制平面节点都创建一个本地etcd，这个本地etcd只与该控制平面节点的kube-apiserver通讯，kube-scheduler和kube-controller-manager也是节点独立的。
介绍一下这种方案的优缺点： 优点：这种控制平面和etcd部署在同一个节点的模式比控制平面和etcd分离部署安装容易并且易于管理副本。 缺点：这种模式存在错误耦合的缺点，当一个节点宕机，etcd和控制平面实例都丢失，只能添加更多的控制平面节点来缓和这个问题。
在高可用集群下需要运行至少3个堆叠控制平面节点。
这是kubeadm高可用的默认实现方案，运行kubeadm init和kubeadm join &amp;ndash;control-plane后本地etcd将被自动创建。 0.2.2 分离式etcd拓扑 分离式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在不同服务器上。
类似于堆叠式架构，每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。不同的地方在于，etcd在不同的host上运行，每个etcd都会和每个控制平面上的kube-apiserver交互。
这种拓扑结构将etcd与控制平面进行了分离。这种方式与堆叠式架构相比可以在etcd的节点丢失以及控制平面的节点丢失上有更高的容错性。
然而，这种拓扑结构在机器数量上对比堆叠式架构需要两倍的数量。需要至少3个etcd节点以及3个控制平面节点。 1 前置安装 1.1 环境相关 安装docker和kubeadm，在每一台机器上都需要安装。
1:ssh可信 2:hosts cat&amp;gt;&amp;gt;/etc/hosts&amp;lt;&amp;lt;EOF 10.57.26.15 k8s-01 10.57.26.7 k8s-02 10.57.26.8 k8s-03 10.57.26.26 k8s-04 10.57.26.11 k8s-05 10.57.26.17 k8s-06 10.57.26.33 k8s-07 EOF host名需要和 hostname /etc/hostname 一致 3:安全相关 setenforce 0 sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config systemctl disable firewalld.service systemctl stop firewalld.service systemctl disable iptables.</description>
    </item>
    
    <item>
      <title>Kubeadm离线安装单master节点k8s</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:11 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</guid>
      <description>1 目标 1.在所有节点上安装docker和kubeadm
2.部署kubernetes master
3.部署容器网络插件
4.部署kubernetes worker
5.部署dashboard可视化插件
6.部署容器存储插件
2 准备(所有安装机器均需执行) 2.1:/etc/hosts中添加ip和host的映射 2.2:master和node做互信 2.3:关闭防火墙和selinux 1) 关闭selinux
sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config  2) 关闭防火墙
systemctl disable firewalld.service systemctl stop firewalld.service  3) 关闭iptables
systemctl disable iptables.service systemctl stop iptables.service  2.4:安装docker 将离线包解压，进docker目录，运行sudo yum install * 安装好后再执行
systemctl start docker systemctl enable docker  将docker服务启动
2.5:私仓搭建 这里选择了docker的registry2来建立私有仓库。
 下载镜像仓库
docker pull registry:2  启动镜像仓库
docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 --restart=always --name registry registry:2  配置阿里云的Docker加速器，加快pull registry镜像</description>
    </item>
    
    <item>
      <title>K8s安装离线依赖库与镜像制作</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</guid>
      <description>一：docker环境下载 1.1 yum仓库添加 vim /etc/yum.repos.d/docker-ce.repo 添加以下内容： [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg  推荐添加阿里云的源，速度非常快。
1.2 docker环境本地下载 yum install --downloadonly --downloaddir=. docker-ce-18.09.4-3.el7  该命令会把docker以及依赖全下载到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
二：kubeadm环境下载 2.1 yum仓库添加 vim /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg  2.2 kubeadm环境本地下载 yum install --downloadonly --downloaddir=. kubeadm  该命令会把kubeadm以及依赖全下载包括kubelet,kubectl到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
三：k8s依赖镜像下载 kubeadm安装时会访问特定的tag，若不存在就会走外网下载，所以kubeadm部署前必须将所需的k8s环境以镜像方式下载好，并导入docker中
3.1 阿里云海外镜像下载配置 由于各种原因，k8s的依赖kube-proxy，kube-apiserver，kube-controller-manager，kube-scheduler，coredns，etcd和pause都难于下载，本人通过阿里云海外镜像下载方式，分享一下。 1) 将github上建立一个项目，github的配置不再赘述。 2) 将所有依赖建立目录，并在每个目录下建立Dockerfile，并输入FROM命令，每个组件的命令如下：
FROM k8s.gcr.io/kube-apiserver:v1.14.2 FROM k8s.gcr.io/kube-proxy:v1.14.2 FROM k8s.gcr.io/kube-controller-manager:v1.14.2 FROM k8s.gcr.io/kube-scheduler:v1.14.2 FROM k8s.</description>
    </item>
    
    <item>
      <title>harbor仓库搭建手册</title>
      <link>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</link>
      <pubDate>Fri, 21 Jun 2019 23:21:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</guid>
      <description>一 harbor介绍 二 环境准备 centos 7.6.1810 docker 18.09.4 docker-compose 1.21.2
2.1 安装docker 2.2 安装docker-compose 下载并解压后，将docker-compose赋权限并放置环境变量下。
三 搭建harbor 3.1 下载离线包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz  下载并解压
3.2 修改harbor配置(http) harbor ui端口默认是80端口，会和大多数应用冲突，所以需要更改，要改两个地方。第一个地方是docker-compose.yml文件，将ports 80:80改成 xxx:80即可，如下图所示： 第二个地方是harbor.yml文件，将http:port: 80更改成需要的，并将hostname改成ip或域名，这个hostname需要与pull和push的前缀一致。如图所示 3.3 生成相关配置并安装 运行 ./prepare以及./install.sh
3.4 启动 docker-compose up -d  3.5 启动后相关容器 正常启动后会有9个容器，如下图所示，其中nginx-photon:v1.8.0容器可以看到端口映射4001-&amp;gt;80 四、客户端docker配置 vi /etc/docker/daemon.json 将&amp;quot;10.57.30.22:4001&amp;quot;添加insecure-registries中 systemctl daemon-reload systemctl restart docker  五、访问 Web UI 并测试 5.1、主页 访问主页http://10.57.30.22:4001 即可看到登陆页面，输入密码后进入，默认账户是admin，密码在harbor.yml中的harbor_admin_password字段。 进入后如下显示： 5.2 创建项目 Harbor 有一个项目的概念，项目名可以理解为 Docker Hub 的用户名，其下可以后很多 images，Harbor 的项目必须登录后方可 push，公有项目和私有项目的区别是对其他用户是否可见 创建项目后，才可以push和pull。</description>
    </item>
    
    <item>
      <title>k8s部署jenkins以及基本使用</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 21 Jun 2019 16:52:58 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>一：Jenkins+Kubernetes方案简介 1.1、传统Jenkins部署的问题 Jenkins 是开源的一套持续集成框架，可以进行大规模的编译、测试和发布的工作，给软件开发团队带来极大的便利性。 Jenkins 的持续集成环境可以是集群化的，主要的运行模式为master-slave模式。 Jenkins 的master为Jenkins系统的控制节点，slave节点负责具体的项目编译测试等工作。 由于大公司里面需要进行编译的工程或者对象非常庞大，因此需要大量的物理节点作为slave,而且这些环节相对固定，可能很难适应其他项目的编译测试，一旦salve节点遭到破坏，需要人为的进行修复甚至重建，非常麻烦。 2.2、与Kubernetes-plugin的结合 Docker的问世，为我们提供了解决方案，使用docker作为jenkins slave节点可以解决slave节点遭到破坏后遇到的问题， 而且大量的工程不是每时每刻同时运行的因此可以在需要时吧docker拉起来进行编译测试，这样就节约了大量的物理节点， 解决了上述问题。 然而使用docker 同样需要集群，因此要用到集群管理工具， swarm 或者kubernetes。Swarm 一般用在小集群上，而且swarm和docker本身的借口完全一致， 因此这里就简单介绍单点docker节点作为slave。 Kubernetes作为大的docker 集群管理工具，当jenkins的工程数量非常大的时候可以用kubernetes, kubernetes的运维相对比docker 和swarm的门槛要高一点。 Jenkins master也可以使用docker来运行或者使用kubernetes来提供高可用的jenkins master。 Kubernetes 主要是用来进行docker 调度运行的，同样因为这一点通过kubernetes插件的配置，不需要管slave节点是否在线，因为kubernetes帮你负责创建和连接。这样就可以做到没有任务的时候没有任务slave节点在线，做到完全的按需启动slave和调度slave。
二、在K8S上构建Jenkins Server 2.1 创建PVC存储 首先需要对jenkins-server创建一个Kubernetes持久化存储卷。为了让Jenkins Server可以具有Fail Over的能力，建议将Jenkins的数据存储到远程存储或者分布式存储上。本文使用分布式存储ceph来模拟存储效果，分布式存储的部署请参考我的《rook+ceph部署分布式文件系统》，指定rook-ceph-block并分配200G空间。
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cicdpvc namespace: cicd labels: app: cicd spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 200Gi  2.2 创建jenkins-server 创建jenkins-server的yaml如下所示
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: jenkins2 namespace: cicd spec: template: metadata: labels: app: jenkins2 spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins2 containers: - name: jenkins image: 10.</description>
    </item>
    
  </channel>
</rss>