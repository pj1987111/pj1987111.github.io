<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ZHY ZONE</title>
    <link>https://pj1987111.github.io/posts/</link>
    <description>Recent content in Posts on ZHY ZONE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Jun 2019 14:05:02 +0800</lastBuildDate>
    
	<atom:link href="https://pj1987111.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>k8s高可用架构</title>
      <link>https://pj1987111.github.io/posts/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:02 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/</guid>
      <description>1 与keepalive+VIP对比 网上大量的人使用KeepAlive+VIP的形式完成高可用，这个方式有两个不好的地方：
其一，受限于使用者的网络，无法适用于SDN网络，比如Aliyun的VPC。
其二，虽然是高可用的，但是流量还是单点的，所有node的的网络I/O都会高度集中于一台机器上（VIP)，一旦集群节点增多，pod增多，单机的网络I/O迟早是网络隐患。
2 两种模式 2.1 堆叠式etcd拓扑 堆叠式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在同一台服务器上，共同被kubeadm所管理。
每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。
每个控制平面节点都创建一个本地etcd，这个本地etcd只与该控制平面节点的kube-apiserver通讯，kube-scheduler和kube-controller-manager也是节点独立的。
介绍一下这种方案的优缺点： 优点：这种控制平面和etcd部署在同一个节点的模式比控制平面和etcd分离部署安装容易并且易于管理副本。 缺点：这种模式存在错误耦合的缺点，当一个节点宕机，etcd和控制平面实例都丢失，只能添加更多的控制平面节点来缓和这个问题。
在高可用集群下需要运行至少3个堆叠控制平面节点。
这是kubeadm高可用的默认实现方案，运行kubeadm init和kubeadm join &amp;ndash;control-plane后本地etcd将被自动创建。 2.2 分离式etcd拓扑 分离式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在不同服务器上。
类似于堆叠式架构，每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。不同的地方在于，etcd在不同的host上运行，每个etcd都会和每个控制平面上的kube-apiserver交互。
这种拓扑结构将etcd与控制平面进行了分离。这种方式与堆叠式架构相比可以在etcd的节点丢失以及控制平面的节点丢失上有更高的容错性。
然而，这种拓扑结构在机器数量上对比堆叠式架构需要两倍的数量。需要至少3个etcd节点以及3个控制平面节点。 3 安装与部署 在接下来的介绍文章中会对这两种模式的安装部署方式做详细介绍，请看</description>
    </item>
    
    <item>
      <title>kubeadm安装高可用k8s集群</title>
      <link>https://pj1987111.github.io/posts/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:00 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>1 前置安装 1.1 环境相关 安装docker和kubeadm，在每一台机器上都需要安装。
1:ssh可信 2:hosts cat&amp;gt;&amp;gt;/etc/hosts&amp;lt;&amp;lt;EOF 10.57.26.15 k8s-01 10.57.26.7 k8s-02 10.57.26.8 k8s-03 10.57.26.26 k8s-04 10.57.26.11 k8s-05 10.57.26.17 k8s-06 10.57.26.33 k8s-07 EOF host名需要和 hostname /etc/hostname 一致 3:安全相关 setenforce 0 sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config systemctl disable firewalld.service systemctl stop firewalld.service systemctl disable iptables.service systemctl stop iptables.service 4:源和代理 echo proxy=http://10.57.22.8:3128/ &amp;gt;&amp;gt; /etc/yum.conf cat&amp;gt;/etc/yum.repos.d/docker-ce.repo&amp;lt;&amp;lt;EOF [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg EOF cat&amp;gt;/etc/yum.repos.d/kubernetes.repo&amp;lt;&amp;lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.</description>
    </item>
    
    <item>
      <title>Hugo&#43;github搭建个人网站</title>
      <link>https://pj1987111.github.io/posts/hugo&#43;github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</link>
      <pubDate>Sat, 22 Jun 2019 22:13:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/hugo&#43;github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</guid>
      <description>一 安装Hugo 在github上找到相应版本github地址，本人是mac，就下载了hugo_0.55.6_macOS-64bit.tar.gz。下载解压后，将hugo添加进环境变量中。
二 在Hugo中写文章 在硬盘中选取合适的存储路径，然后命令行中使用如下指令生成网页本地文档：
hugo new site personal-site  由此可得到如下文件目录：
personal-site ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── static └── themes 常用目录用处如下
   子目录名称 功能     archetypes 新文章默认模板   config.toml Hugo配置文档   content 存放所有Markdown格式的文章   layouts 存放自定义的view，可为空   static 存放图像、CNAME、css、js等资源，发布后该目录下所有资源将处于网页根目录   themes 存放下载的主题    使用下面的命令生成新的文章草稿：
hugo new posts/first-post.md 在content目录中会自动以archetypes/default.md为模板在content/posts目录下生成一篇名为first-post.md的文章草稿：
--- title: &amp;quot;First Post&amp;quot; date: 2017-12-27T23:15:53-05:00 draft: true ---  我们可以加一个标题在下面并去掉标记为草稿的这一行：draft: true</description>
    </item>
    
    <item>
      <title>Kubeadm离线安装单master节点k8s</title>
      <link>https://pj1987111.github.io/posts/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:11 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</guid>
      <description>1 目标 1.在所有节点上安装docker和kubeadm
2.部署kubernetes master
3.部署容器网络插件
4.部署kubernetes worker
5.部署dashboard可视化插件
6.部署容器存储插件
2 准备(所有安装机器均需执行) 2.1:/etc/hosts中添加ip和host的映射 2.2:master和node做互信 2.3:关闭防火墙和selinux 1) 关闭selinux
sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config  2) 关闭防火墙
systemctl disable firewalld.service systemctl stop firewalld.service  3) 关闭iptables
systemctl disable iptables.service systemctl stop iptables.service  2.4:安装docker 将离线包解压，进docker目录，运行sudo yum install * 安装好后再执行
systemctl start docker systemctl enable docker  将docker服务启动
2.5:私仓搭建 这里选择了docker的registry2来建立私有仓库。
 下载镜像仓库
docker pull registry:2  启动镜像仓库
docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 --restart=always --name registry registry:2  配置阿里云的Docker加速器，加快pull registry镜像</description>
    </item>
    
    <item>
      <title>K8s安装离线依赖库与镜像制作</title>
      <link>https://pj1987111.github.io/posts/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</guid>
      <description>一：docker环境下载 1.1 yum仓库添加 vim /etc/yum.repos.d/docker-ce.repo 添加以下内容： [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg  推荐添加阿里云的源，速度非常快。
1.2 docker环境本地下载 yum install --downloadonly --downloaddir=. docker-ce-18.09.4-3.el7  该命令会把docker以及依赖全下载到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
二：kubeadm环境下载 2.1 yum仓库添加 vim /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg  2.2 kubeadm环境本地下载 yum install --downloadonly --downloaddir=. kubeadm  该命令会把kubeadm以及依赖全下载包括kubelet,kubectl到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
三：k8s依赖镜像下载 kubeadm安装时会访问特定的tag，若不存在就会走外网下载，所以kubeadm部署前必须将所需的k8s环境以镜像方式下载好，并导入docker中
3.1 阿里云海外镜像下载配置 由于各种原因，k8s的依赖kube-proxy，kube-apiserver，kube-controller-manager，kube-scheduler，coredns，etcd和pause都难于下载，本人通过阿里云海外镜像下载方式，分享一下。 1) 将github上建立一个项目，github的配置不再赘述。 2) 将所有依赖建立目录，并在每个目录下建立Dockerfile，并输入FROM命令，每个组件的命令如下：
FROM k8s.gcr.io/kube-apiserver:v1.14.2 FROM k8s.gcr.io/kube-proxy:v1.14.2 FROM k8s.gcr.io/kube-controller-manager:v1.14.2 FROM k8s.gcr.io/kube-scheduler:v1.14.2 FROM k8s.</description>
    </item>
    
    <item>
      <title>harbor仓库搭建手册</title>
      <link>https://pj1987111.github.io/posts/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</link>
      <pubDate>Fri, 21 Jun 2019 23:21:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</guid>
      <description>一 harbor介绍 二 环境准备 centos 7.6.1810 docker 18.09.4 docker-compose 1.21.2
2.1 安装docker 2.2 安装docker-compose 下载并解压后，将docker-compose赋权限并放置环境变量下。
三 搭建harbor 3.1 下载离线包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz  下载并解压
3.2 修改harbor配置(http) harbor ui端口默认是80端口，会和大多数应用冲突，所以需要更改，要改两个地方。第一个地方是docker-compose.yml文件，将ports 80:80改成 xxx:80即可，如下图所示： 第二个地方是harbor.yml文件，将http:port: 80更改成需要的，并将hostname改成ip或域名，这个hostname需要与pull和push的前缀一致。如图所示 3.3 生成相关配置并安装 运行 ./prepare以及./install.sh
3.4 启动 docker-compose up -d  3.5 启动后相关容器 正常启动后会有9个容器，如下图所示，其中nginx-photon:v1.8.0容器可以看到端口映射4001-&amp;gt;80 四、客户端docker配置 vi /etc/docker/daemon.json 将&amp;quot;10.57.30.22:4001&amp;quot;添加insecure-registries中 systemctl daemon-reload systemctl restart docker  五、访问 Web UI 并测试 5.1、主页 访问主页http://10.57.30.22:4001 即可看到登陆页面，输入密码后进入，默认账户是admin，密码在harbor.yml中的harbor_admin_password字段。 进入后如下显示： 5.2 创建项目 Harbor 有一个项目的概念，项目名可以理解为 Docker Hub 的用户名，其下可以后很多 images，Harbor 的项目必须登录后方可 push，公有项目和私有项目的区别是对其他用户是否可见 创建项目后，才可以push和pull。</description>
    </item>
    
    <item>
      <title>k8s部署jenkins以及基本使用</title>
      <link>https://pj1987111.github.io/posts/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 21 Jun 2019 16:52:58 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>一：Jenkins+Kubernetes方案简介 1.1、传统Jenkins部署的问题 Jenkins 是开源的一套持续集成框架，可以进行大规模的编译、测试和发布的工作，给软件开发团队带来极大的便利性。 Jenkins 的持续集成环境可以是集群化的，主要的运行模式为master-slave模式。 Jenkins 的master为Jenkins系统的控制节点，slave节点负责具体的项目编译测试等工作。 由于大公司里面需要进行编译的工程或者对象非常庞大，因此需要大量的物理节点作为slave,而且这些环节相对固定，可能很难适应其他项目的编译测试，一旦salve节点遭到破坏，需要人为的进行修复甚至重建，非常麻烦。 2.2、与Kubernetes-plugin的结合 Docker的问世，为我们提供了解决方案，使用docker作为jenkins slave节点可以解决slave节点遭到破坏后遇到的问题， 而且大量的工程不是每时每刻同时运行的因此可以在需要时吧docker拉起来进行编译测试，这样就节约了大量的物理节点， 解决了上述问题。 然而使用docker 同样需要集群，因此要用到集群管理工具， swarm 或者kubernetes。Swarm 一般用在小集群上，而且swarm和docker本身的借口完全一致， 因此这里就简单介绍单点docker节点作为slave。 Kubernetes作为大的docker 集群管理工具，当jenkins的工程数量非常大的时候可以用kubernetes, kubernetes的运维相对比docker 和swarm的门槛要高一点。 Jenkins master也可以使用docker来运行或者使用kubernetes来提供高可用的jenkins master。 Kubernetes 主要是用来进行docker 调度运行的，同样因为这一点通过kubernetes插件的配置，不需要管slave节点是否在线，因为kubernetes帮你负责创建和连接。这样就可以做到没有任务的时候没有任务slave节点在线，做到完全的按需启动slave和调度slave。
二、在K8S上构建Jenkins Server 2.1 创建PVC存储 首先需要对jenkins-server创建一个Kubernetes持久化存储卷。为了让Jenkins Server可以具有Fail Over的能力，建议将Jenkins的数据存储到远程存储或者分布式存储上。本文使用分布式存储ceph来模拟存储效果，分布式存储的部署请参考我的《rook+ceph部署分布式文件系统》，指定rook-ceph-block并分配200G空间。
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cicdpvc namespace: cicd labels: app: cicd spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 200Gi  2.2 创建jenkins-server 创建jenkins-server的yaml如下所示
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: jenkins2 namespace: cicd spec: template: metadata: labels: app: jenkins2 spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins2 containers: - name: jenkins image: 10.</description>
    </item>
    
  </channel>
</rss>