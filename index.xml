<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ZHY ZONE</title>
    <link>https://pj1987111.github.io/</link>
    <description>Recent content on ZHY ZONE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Aug 2019 11:17:24 +0800</lastBuildDate>
    
	<atom:link href="https://pj1987111.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NFS服务的搭建</title>
      <link>https://pj1987111.github.io/posts/linux/nfs/</link>
      <pubDate>Sat, 10 Aug 2019 11:17:24 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/linux/nfs/</guid>
      <description>1 安装nfs和rpcbind yum -y install nfs-utils rpcbind  2 配置NFS存储路径 nfs 的配置文件 /etc/expots
默认为空
mkdir -p /opt/nfsdata/ cat &amp;gt;/etc/exports &amp;lt;&amp;lt;EOF /opt/nfsdata/ *(rw,no_root_squash,no_all_squash,sync,anonuid=501,anongid=501) EOF  注1:设置的路径必须手动建好，不然会报错无目录 注2：配置文件说明：
/opt/nfsdata 为共享目录
 可以为一个网段，一个IP，也可以是域名，域名支持通配符，*表示所有网段  rw：read-write，可读写；
ro：read-only，只读；
sync：文件同时写入硬盘和内存；
async：文件暂存于内存，而不是直接写入内存；
no_root_squash：NFS客户端连接服务端时如果使用的是root的话，那么对服务端分享的目录来说，也拥有root权限。显然开启这项是不安全的。
root_squash：NFS客户端连接服务端时如果使用的是root的话，那么对服务端分享的目录来说，拥有匿名用户权限，通常他将使用nobody或nfsnobody身份；
all_squash：不论NFS客户端连接服务端时使用什么用户，对服务端分享的目录来说都是拥有匿名用户权限；
anonuid：匿名用户的UID值，可以在此处自行设定。
anongid：匿名用户的GID值。
如果更新配置文件/etc/expots需要重启nfs.service 3 启动NFS服务 把运行rpcbind.service和nfs.service服务
systemctl restart rpcbind.service systemctl restart nfs.service  启动起来之后，你会看见他们分别监听了如下端口： 执行命令systemctl enable rpcbind.service nfs.service将上述两个服务设置为开机自启动。
4 客户端挂载NFS 客户端上查看可挂载的目录：
showmount -e &amp;lt;nfs server地址&amp;gt;  客户端挂载
mkdir -p /mnt/nfsdata mount -t nfs 10.</description>
    </item>
    
    <item>
      <title>设置RBAC规则来实现多用户权限隔离访问</title>
      <link>https://pj1987111.github.io/posts/k8s/rbac_geli/</link>
      <pubDate>Fri, 09 Aug 2019 11:18:23 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/rbac_geli/</guid>
      <description>1 AAA和BBB产品线容器的环境准备 1.1 两个产品线的权限隔离说明 假如我们公司有productline-aaa和productline-bbb两个产品线，每个产品线都有各自相应的负责人。要求每个产品线的负责人只能管理自己产品线的容器，实现效果如下： 为了方便演示，我这里建productline-aaa和productline-bbb两个不同的命名空间，并分别在每个命名空间中创建几个容器和service。
1.2 创建两个产品线的容器环境 创建productline-aaa命名空间，并在productline-aaa命名空间中创建2个productline-aaa-testweb容器和productline-aaa-testweb服务，productline-aaa同理。
编写yaml代码 aaa产线
apiVersion: v1 kind: Namespace metadata: name: productline-aaa --- apiVersion: v1 kind: ReplicationController metadata: name: productline-aaa-testweb namespace: productline-aaa labels: name: productline-aaa-testweb spec: replicas: 2 selector: name: productline-aaa-testweb template: metadata: labels: name: productline-aaa-testweb spec: containers: - name: productline-aaa-testweb image: 10.57.26.15:4000/guestbook-php-frontend env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: productline-aaa-testweb namespace: productline-aaa labels: name: productline-aaa-testweb spec: ports: - port: 80 selector: name: productline-aaa-testweb  bbb产线</description>
    </item>
    
    <item>
      <title>常用组件命令</title>
      <link>https://pj1987111.github.io/posts/hadoop/%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Tue, 06 Aug 2019 19:46:19 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/hadoop/%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E5%91%BD%E4%BB%A4/</guid>
      <description>1 hive 特点：不能更新，全量读 其中10000端口是hiveserver2的端口，从cdh中可以查看，如下： beeline -u jdbc:hive2://cdh217:10000  或者进到master里面，输入hive进去，操作和beeline一样
只能查text表，parquet表要到平台去查，因为要解压缩。
2 hbase 特点：更新快，只能rowkey读速度快 get &amp;lsquo;test&amp;rsquo;, &amp;ldquo;\x00\x00\x02\x08&amp;rdquo; get &amp;lsquo;test&amp;rsquo;, &amp;ldquo;\x00\x00\x02\x08&amp;rdquo;,&amp;lsquo;cf1:dep_id&amp;rsquo;
3 hdfs 以hdfs://tdhdfs/user/hive/warehouse/testflink.db/kafka_to_hive5为例子
注意，若客户端要访问该地址的话，需要把tdhdfs添加进/etc/hosts中，tdhdfs是namenode地址，若配置ha的话写任意一个即可。
4 zookeeper 5 kafka 1 producer
netstat -pant | grep 9092可以查看，一般是zk的地址
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test  2 consumer
有时候第一个命令没有，用第二个
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning  或者
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning  5.3 kafka-topics.sh 注:&amp;ndash;zookeeper后参数从config/server.properties文件中zookeeper.connect参数获取
1 展示topic 展示topic名
bin/kafka-topics.sh --zookeeper localhost:2181 --list  2 描述topic 可以看到topic的分片数、leader、副本等信息</description>
    </item>
    
    <item>
      <title>Redis哨兵模式快速安装以及测试</title>
      <link>https://pj1987111.github.io/posts/database/redis_install/</link>
      <pubDate>Fri, 02 Aug 2019 11:59:41 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/database/redis_install/</guid>
      <description>0 基础方面 基础方面可以参考文档： https://www.cnblogs.com/kevingrace/p/9004460.html
1 机器 master: k8s-01 slave1: k8s-02 slave2: k8s-03
2 安装(所有节点执行) 以下以脚本方式安装 install_redis.sh
REDIS_PATH=/root REDIS_TAG=redis-5.0.5.tar.gz REDIS_TAGNAME=redis-5.0.5 REDIS_INSTALL_PATH=/root/redis_sentinel function install_redis () { ################################################################################################# cd $REDIS_PATH echo &amp;quot; $REDIS_TAG&amp;quot; if [ ! -f &amp;quot;$REDIS_TAG&amp;quot; ]; then echo download wget http://download.redis.io/releases/$REDIS_TAG fi mkdir -p $REDIS_INSTALL_PATH echo &amp;quot;=====end mkdir -p $REDIS_INSTALL_PATH======&amp;quot; tar -zxf $REDIS_PATH/$REDIS_TAG -C $REDIS_INSTALL_PATH echo &amp;quot;=====end tar -zxf $REDIS_PATH/$REDIS_TAG -C $REDIS_INSTALL_PAT =====&amp;quot; cd $REDIS_INSTALL_PATH/$REDIS_TAGNAME make PREFIX=/usr/local/redis install echo &amp;quot;=====end make PREFIX=/usr/local/redis install=====&amp;quot; mkdir -p /usr/local/redis/{etc,var} ################################################################################################# } install_redis  在master节点上安装后，将脚本(或者加上安装包)复制到slave节点上，并在slave节点上执行脚本</description>
    </item>
    
    <item>
      <title>kdc配置HA</title>
      <link>https://pj1987111.github.io/posts/kerberos/2.kdc_ha/</link>
      <pubDate>Thu, 01 Aug 2019 10:48:57 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/kerberos/2.kdc_ha/</guid>
      <description>1 机器 Kdc主节点：k8s-01
kdc从节点：k8s-02
2 主节点修改 2.1 主节点修改配置 /etc/krb5.conf
[realms] TONGDUN.COM = { kdc = k8s-01 kdc = k8s-02 …（如果有多从kdc，一行一个） admin_server = k8s-01 }  2.2 添加principal kadmin.local
addprinc -randkey host/k8s-01 addprinc -randkey host/k8s-02  //将主、从节点的princ整合到krb5.keytab中
ktadd -k /etc/krb5.keytab host/k8s-01 ktadd -k /etc/krb5.keytab host/k8s-02  3 从节点搭建 从节点安装参考《KDC服务安装及配置》
3.1 将master上配置同步到从节点 将master几个配置拷贝到从服务器：文件: krb5.conf、kdc.conf、kadmin5.acl、master key stash file
 scp /etc/krb5.conf root@k8s-02:/etc scp /etc/krb5.keytab root@k8s-02:/etc scp /usr/local/krb5/var/krb5kdc/kdc.conf root@k8s-02:/usr/local/krb5/var/krb5kdc/ scp /usr/local/krb5/var/krb5kdc/kadm5.acl root@k8s-02:/usr/local/krb5/var/krb5kdc/ scp /usr/local/krb5/var/krb5kdc/.</description>
    </item>
    
    <item>
      <title>KDC服务安装及配置</title>
      <link>https://pj1987111.github.io/posts/kerberos/1.kdc_install/</link>
      <pubDate>Thu, 01 Aug 2019 10:48:39 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/kerberos/1.kdc_install/</guid>
      <description>1 Kerberos介绍 Kerberos这一名词来源于希腊神话“三个头的狗——地狱之门守护者”。
系统设计上采用客户端/服务器结构与DES加密技术，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止replay攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统。
支持SSO(Single Sign On)，用户只需输入一次身份验证信息就可以凭借此验证获得的票据(ticket-granting ticket)访问多个服务。
由于在每个Client和Service之间建立了共享密钥，使得该协议具有相当的安全性。
2 认证原理解析 2.1 准备阶段 先看下图：
这里出现的角色有三个分别是：
 KDC （Key Distribution Center） Client Service  其中KDC有两个服务：
 AS (Authentication Server)： 认证服务，用于签发“Ticket-Granting Tickets” (TGT) TGS(Ticket Granting Server)： 许可证服务，用于签发service tickets(各种服务的tickets)  密钥两个：
 KDC与Client之间的共享密钥（密钥A,其实就是用户密码） KDC与Service之间的共享密钥(密钥B)  2.2 认证过程 第一步： 用户发送自己的用户信息给KDC，KDC访问AS服务，获得TGT，并用密钥A加密TGT以及一个Session Key给用户。用户得到加密数据后，使用密钥A解密得到TGT和Session Key（这里简称为SK）。
其中，TGT用于第二步请求各种服务的tickets;SK主要用于Service对Client的身份鉴别，在步骤二中会体现。
第二步： 如图：  Client将之前获得TGT和要请求的服务信息(服务名等)发送给KDC，认证用户合法后，KDC中的TGS将SK(第一步中产生)和用户名，用户地址（IP），服务名，有效期, 时间戳一起包装成一个Ticket，并用密钥B加密，发送给Client； 此时Client没有密钥B所以他无法查看Ticket中的内容，于是Client将Ticket直接转发给Service。 同时Client将自己的用户名，用户地址（IP）打包成Authenticator，用之前获得的SK加密也发送给Service。 Service 收到Ticket后利用它与KDC之间的密钥B将Ticket中的信息解密出来，从而获得SK和用户名，用户地址（IP），服务名，有效期。然后再用SK将Authenticator解密从而获得用户名，用户地址（IP）将其与之前Ticket中解密出来的用户名，用户地址（IP）做比较，从而验证Client的身份。 如果Service有返回结果，将其返回给Client。  2 KDC安装 本文档中将KDC服务安装在Cloudera Manager Server所在服务器上（KDC服务可根据自己需要安装在其他服务器）
测试环境测试时，kdc安装在dp199机器上，cdh在dp197上。
2.1 在Cloudera Manager服务器上安装KDC服务（可以在别的机器上装kdc） 2.</description>
    </item>
    
    <item>
      <title>Nginx&#43;Ingress-controller解决服务暴露和负载均衡</title>
      <link>https://pj1987111.github.io/posts/k8s/nginx&#43;ingress-controller%E8%A7%A3%E5%86%B3l7%E5%A4%96%E7%BD%91web%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Sun, 28 Jul 2019 12:04:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/nginx&#43;ingress-controller%E8%A7%A3%E5%86%B3l7%E5%A4%96%E7%BD%91web%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>1 环境说明和准备 Ingress 使用开源的反向代理负载均衡器来实现对外暴露服务，比如 Nginx、Apache、Haproxy等。Nginx Ingress 一般有三个组件组成：
Nginx：反向代理负载均衡器
Ingress Controller ：Ingress Controller 可以理解为控制器，它通过不断的跟 Kubernetes API 交互，实时获取后端 Service、Pod 等的变化，比如新增、删除等，然后结合 Ingress 定义的规则生成配置，然后动态更新上边的 Nginx 负载均衡器，并刷新使配置生效，来达到服务自动发现的作用。
Ingress： Ingress 则是定义规则，通过它定义某个域名的请求过来之后转发到集群中指定的 Service。它可以通过 Yaml 文件定义，可以给一个或多个 Service 定义一个或多个 Ingress 规则。
以上三者有机的协调配合起来，就可以完成 Kubernetes 集群服务的暴露。
拿前后端分离的frontend和testweb两个web服务为例，大致拓扑如下： 2 安装部署 2.1 安装部署default-backend 创建Deployment，副本为1，并创建service绑定pod。配置文件如下所示：
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: default-http-backend labels: k8s-app: default-http-backend namespace: kube-system spec: replicas: 1 template: metadata: labels: k8s-app: default-http-backend spec: terminationGracePeriodSeconds: 60 containers: - name: default-http-backend # Any image is permissable as long as: # 1.</description>
    </item>
    
    <item>
      <title>docker和k8s小技巧</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%B0%8F%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Fri, 26 Jul 2019 10:44:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%B0%8F%E6%8A%80%E5%B7%A7/</guid>
      <description> 1 设置默认ns 1.1 问题描述： 之前装过jhub，后来删除，jhub创建时候会修改namespace从default到jhub。后来导致所有缺省namespace的应用都装在jhub上，手动删除jhub的ns后新pod创建不上，报错找不到jhub。
1.2 解决方式： 输入kubectl config view命令查看与context相关的配置，如下图所示，可见name是kubernetes-admin@kubernetes的namespace是jhub。 输入kubectl config set-context $(kubectl config current-context) &amp;ndash;namespace=default 命令将namespace的jhub改成default即可。 $(kubectl config current-context)的输出是kubernetes-admin@kubernetes。修改后输出结果如下图所示。 2 一些有用的docker镜像 2.1 aerospike aerospike管理器 
docker run -ti --name aerospike-asadm --rm aerospike/aerospike-tools asadm --host 10.57.30.214 --no-config-file  aerospike工具，查看sql用 
docker run -ti --name aerospike-aql --rm aerospike/aerospike-tools aql -h 10.57.30.214 --no-config-file  2.2 mssql linux版本 注：密码必须有大小写数字超过10位，不然镜像会自动退出。 例子中将镜像中1433端口映射到宿主机的1401端口，并将镜像中的/var/opt/mssql路径映射到宿主机/var/mssql路径下，设置&amp;ndash;name sql1防止镜像重建
docker run -e &#39;ACCEPT_EULA=Y&#39; -e &#39;MSSQL_SA_PASSWORD=YourStrong!Passw0rd&#39; -p 1401:1433 -v /var/mssql:/var/opt/mssql --name sql1 -d microsoft/mssql-server-linux:2017-latest  </description>
    </item>
    
    <item>
      <title>不错的参考文章</title>
      <link>https://pj1987111.github.io/posts/k8s/%E4%B8%8D%E9%94%99%E7%9A%84%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0/</link>
      <pubDate>Tue, 23 Jul 2019 21:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/%E4%B8%8D%E9%94%99%E7%9A%84%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0/</guid>
      <description>1 kubernetes-crash-course系列 0 - Kubernetes Overview
1 - Kubernetes Objects 
2 - Kubernetes Pods
3 - Kubernetes Services and Ingress
4 - Kubernetes Storage</description>
    </item>
    
    <item>
      <title>mysql</title>
      <link>https://pj1987111.github.io/posts/k8s/mysql/</link>
      <pubDate>Tue, 23 Jul 2019 20:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/mysql/</guid>
      <description> 1 进入mysql的pod，并创建数据库，表，并插入数据。 执行 kubectl delete pod mysql-6d6cb6c594-t6k4w 后，删除老的pod，deploy会新建同样的pod做恢复。
再进入新的pod中，show database，可以发现之前那个mysql pod建立的数据库，表和数据都不存在，如图所示： </description>
    </item>
    
    <item>
      <title>k8s部署dashboard与踩坑</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2dashboard%E4%B8%8E%E8%B8%A9%E5%9D%91/</link>
      <pubDate>Tue, 23 Jul 2019 13:42:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2dashboard%E4%B8%8E%E8%B8%A9%E5%9D%91/</guid>
      <description>1 简介 使用kube-dashboard来完成日常的一些k8s集群运维工作。
Dashboard是Kubernetes社区官方开发的仪表板，有了仪表板后管理者就能够透过 Web-based 方式来管理 Kubernetes 集群，除了提升管理方便，也让资源可视化，让人更直觉看见系统信息的呈现结果。
下面我们就来看如何基于部署kube-dashboard。
2 安装kube-dashboard组件 在安装之前，先通过删除namespace来清除之前版本的内容，通过命令
kubectl delete ns kubernetes-dashboard  下载yaml文件，使用目前最新的v2.0.0-beta2版本，很好的适应k8s-1.15.0版本。 需要修改两处地方： * 修改镜像地址 sed命令中第一个匹配 * 默认配置文件中ClusterRole为kubernetes-dashboard权限过低，修改成cluster-admin sed命令中第二个匹配，上一行满足替换下一行的模式。 为何更改，下文将详细介绍。
#官网下载yaml curl -o dashboard-v2.0.0-beta2.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta2/aio/deploy/recommended.yaml #更改镜像地址，改成私仓的地址；并将clusterrolebinding附近name: kubernetes-dashboard改成name: cluster-admin以获得管理员权限 sed -i &amp;quot;s/kubernetesui/10.57.26.15:4000/g;/kind: ClusterRole/{n;s/name: kubernetes-dashboard/name: cluster-admin/g}&amp;quot; dashboard-v2.0.0-beta2.yaml #安装 kubectl create -f dashboard-v2.0.0-beta2.yaml  然后可以通过查看安装组件，以及部署状态。 3 dashboard访问 这是我认为dashboard部署的重点，也是坑比较多的地方，我在里面陷了很久，在此分享一下。 我尝试了4种访问dashboard服务的方式，其他服务都可以借鉴。
3.1 kubectl proxy kubectl proxy 的原理是将机器与kubernetes API Server之间做一个代理，默认情况下，只能从本机访问。 可以使用kubectl cluster-info命令检查配置是否正确，集群是否可以访问。 启动代理只需执行如下命令即可：
$ kubectl proxy Starting to serve on 127.</description>
    </item>
    
    <item>
      <title>Linux Shell 脚本攻略笔记</title>
      <link>https://pj1987111.github.io/posts/linux/linux-shell-%E8%84%9A%E6%9C%AC%E6%94%BB%E7%95%A5%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Mon, 15 Jul 2019 16:12:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/linux/linux-shell-%E8%84%9A%E6%9C%AC%E6%94%BB%E7%95%A5%E7%AC%94%E8%AE%B0/</guid>
      <description>Linux Shell 脚本攻略笔记[速查] Linux Shell 脚本攻略的笔记，markdown 编写，可以速查(ctrl+f)
资源[](#资源) sed 简明教程
awk 简明教程
shell script[](#shell-script) #!/bin/bash # do something  run shell script[](#run-shell-script) sh script.sh or chmod a+x script.sh ./script.sh # 会读取首行的解释器, 执行  cmd
cmd1; cmd2 or cmd1 cmd2  echo[](#echo) echo 的功能正如其名，就是基于标准输出打印一段文本
echo &amp;quot;welcome to bash&amp;quot; echo welcome to bash  使用不带引号的 echo 时，无法显示分号
使用单引号 echo 时，bash 不会对单引号中变量求值 ‘\$var’
echo 中转义换行符
默认情况，echo 将换行标志追加到文本尾部，可以忽略结尾换行符
echo -n &#39;test\n&#39;  对字符串进行转义</description>
    </item>
    
    <item>
      <title>Keepalived&#43;HAproxy实现高可用负载均衡</title>
      <link>https://pj1987111.github.io/posts/linux/keepalived&#43;haproxy%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Fri, 05 Jul 2019 22:12:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/linux/keepalived&#43;haproxy%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>1 介绍 Keepalived是一个类似于layer3, 4 &amp;amp; 5交换机制的软件，也就是我们平时说的第3层、第4层和第5层交换。Keepalived的作用是检测web服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的web服务器从系统中剔除，当web服务器工作正常后Keepalived自动将web服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的web服务器
HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案。HAProxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。HAProxy运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。
这里我利用HAproxy对nginx服务器进行负载，然后用Keepalived对HAproxy进行监控：
（主）服务器A：dataocean-04 （从）服务器B：dataocean-03 （VIP)：10.10.51.230
Keepalived监控A、B上的HAproxy，利用Keepalived的VIP漂移技术，若A、B上的HAprox都工作正常，则VIP与优先级别高的服务器（主服务器）绑定，当主服务器当掉时，则与从服务器绑定，而VIP则是暴露给外部访问的ip；HAproxy利用Keepalived生产的VIP对应用所在的nginx进行高可用保障。
部署图如下图所示。 2 HAProxy安装 2.1 编译安装 tar zxvf haproxy-1.7.8.tar.gz cd haproxy-1.7.8 make TARGET=linux2628 PREFIX=/usr/local/haproxy make install PREFIX=/usr/local/haproxy  2.2 配置 2.2.1 编写配置文件 默认安装目录下没有配置文件，只有”doc”，“sbin”，“share”三个目录，可手工创建目录及配置文件； 配置文件中使用的应用程序ip是10.57.26.5，实际使用时需要修改。
mkdir -p /usr/local/haproxy/etc cat&amp;gt;&amp;gt;/usr/local/haproxy/etc/haproxy.cfg&amp;lt;&amp;lt;EOF #全局配置, 用于设定义全局参数, 属于进程级的配置, 通常与操作系统配置有关. global #定义全局日志, 配置在本地, 通过local0 输出, 默认是info级别，可配置两条 log 127.0.0.1 local0 warning #定义日志级别【error warning info debug】 #log 127.0.0.1 local1 info #运行路径 chroot /usr/local/haproxy #PID 文件存放路径 pidfile /var/run/haproxy.pid #设置每haproxy进程的最大并发连接数, 其等同于命令行选项“-n”; “ulimit -n”自动计算的结果参照此参数设定.</description>
    </item>
    
    <item>
      <title>sed命令</title>
      <link>https://pj1987111.github.io/posts/linux/sed/</link>
      <pubDate>Tue, 02 Jul 2019 19:26:07 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/linux/sed/</guid>
      <description>1 sed字符串替换  sed替换的基本语法为:
sed &#39;s/原字符串/替换字符串/&#39;  单引号里面,s表示替换,三根斜线中间是替换的样式,特殊字符需要使用反斜线”\”进行转义。
 单引号” ‘ ’”是没有办法用反斜线”\”转义的,这时候只要把命令中的单引号改为双引号就行了,格式如下：
# 要处理的字符包含单引号 sed &amp;quot;s/原字符串包含&#39;/替换字符串包含&#39;/&amp;quot;   命令中的三根斜线分隔符可以换成别的符号,有时候替换目录字符串的时候有较多斜线，这个时候换成其它的分割符是较为方便,只需要紧跟s定义即可。
# 将分隔符换成问号”?”: sed &#39;s?原字符串?替换字符串?&#39;   可以在末尾加g替换每一个匹配的关键字,否则只替换每行的第一个,例如:
# 替换所有匹配关键字 sed &#39;s/原字符串/替换字符串/g&#39;   一些特殊字符的使用 ^ 表示行首 $符号如果在引号中表示行尾，但是在引号外却表示末行(最后一行)
# 注意这里的 &amp;quot; &amp;amp; &amp;quot; 符号，如果没有 “&amp;amp;”，就会直接将匹配到的字符串替换掉 sed &#39;s/^/添加的头部&amp;amp;/g&#39; #在所有行首添加 sed &#39;s/$/&amp;amp;添加的尾部/g&#39; #在所有行末添加 sed &#39;2s/原字符串/替换字符串/g&#39;　#替换第2行 sed &#39;$s/原字符串/替换字符串/g&#39; #替换最后一行 sed &#39;2,5s/原字符串/替换字符串/g&#39; #替换2到5行 sed &#39;2,$s/原字符串/替换字符串/g&#39; #替换2到最后一行   批量替换字符串 c sed -i &amp;quot;s/查找字段/替换字段/g&amp;quot; `grep 查找字段 -rl 路径` sed -i &amp;quot;s/oldstring/newstring/g&amp;quot; `grep oldstring -rl yourdir   sed处理过的输出是直接输出到屏幕上的,使用参数”i”直接在文件中替换。</description>
    </item>
    
    <item>
      <title>kubeedge安装与部署</title>
      <link>https://pj1987111.github.io/posts/kubeedge/kubeedge%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:04 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/kubeedge/kubeedge%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/</guid>
      <description>1 设备定义 cloud node 10.57.26.15 edge node 10.57.26.26
2 环境依赖配置 1:安装k8s 这一步不再赘述，需要注意的是，官方文档里推荐打开apiserver 8080端口，但是6443安全端口也是可以的。 2:配置Go和kubeedge环境(云端和边缘端都需要安装)
#创建kubeedge源码存放跟路径 mkdir -p kubeedge/src #设置GOPATH export GOPATH=/root/kubeedge #下载go lang包 wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz #解压 tar -xvf go1.10.3.linux-amd64.tar.gz #移动到/usr/local目录下 mv go /usr/local #配置GOROOT环境变量 export GOROOT=/usr/local/go #更新PATH环境变量 export PATH=$GOPATH/bin:$GOROOT/bin:$PATH #下载kubeedge源码于GOPATH路径 git clone https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge  3 生成证书 该步骤将生成kubeedge需要的证书，包含rootCA证书和cert/key对。 同样的cert/key对可以同时被cloud和edge端使用。 运行以下命令生成证书。
$GOPATH/src/github.com/kubeedge/kubeedge/build/tools/certgen.sh genCertAndKey edge  cerrt和 key将被生成在/etc/kubeedge/ca 和/etc/kubeedge/certs 下
4 拷贝证书到边缘节点 在边缘节点上，执行以下命令
mkdir -p /etc/kubeedge/ca mkdir -p /etc/kubeedge/certs scp -r root@node01:/etc/kubeedge/ca /etc/kubeedge scp -r root@node01:/etc/kubeedge/certs /etc/kubeedge  5 编译和运行云端 1 修改modules.</description>
    </item>
    
    <item>
      <title>kubeadm安装高可用k8s集群</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:00 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>0 k8s高可用架构 在介绍部署方式前，先介绍一下k8s的高可用架构。
0.1 与keepalive+VIP对比 网上大量的人使用KeepAlive+VIP的形式完成高可用，这个方式有两个不好的地方：
其一，受限于使用者的网络，无法适用于SDN网络，比如Aliyun的VPC。
其二，虽然是高可用的，但是流量还是单点的，所有node的的网络I/O都会高度集中于一台机器上（VIP)，一旦集群节点增多，pod增多，单机的网络I/O迟早是网络隐患。
0.2 两种模式 0.2.1 堆叠式etcd拓扑 堆叠式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在同一台服务器上，共同被kubeadm所管理。
每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。
每个控制平面节点都创建一个本地etcd，这个本地etcd只与该控制平面节点的kube-apiserver通讯，kube-scheduler和kube-controller-manager也是节点独立的。
介绍一下这种方案的优缺点： 优点：这种控制平面和etcd部署在同一个节点的模式比控制平面和etcd分离部署安装容易并且易于管理副本。 缺点：这种模式存在错误耦合的缺点，当一个节点宕机，etcd和控制平面实例都丢失，只能添加更多的控制平面节点来缓和这个问题。
在高可用集群下需要运行至少3个堆叠控制平面节点。
这是kubeadm高可用的默认实现方案，运行kubeadm init和kubeadm join &amp;ndash;control-plane后本地etcd将被自动创建。 0.2.2 分离式etcd拓扑 分离式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在不同服务器上。
类似于堆叠式架构，每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。不同的地方在于，etcd在不同的host上运行，每个etcd都会和每个控制平面上的kube-apiserver交互。
这种拓扑结构将etcd与控制平面进行了分离。这种方式与堆叠式架构相比可以在etcd的节点丢失以及控制平面的节点丢失上有更高的容错性。
然而，这种拓扑结构在机器数量上对比堆叠式架构需要两倍的数量。需要至少3个etcd节点以及3个控制平面节点。 1 前置安装 1.1 环境相关 安装docker和kubeadm，在每一台机器上都需要安装。
1:ssh可信 2:hosts cat&amp;gt;&amp;gt;/etc/hosts&amp;lt;&amp;lt;EOF 10.57.26.15 k8s-01 10.57.26.7 k8s-02 10.57.26.8 k8s-03 10.57.26.26 k8s-04 10.57.26.11 k8s-05 10.57.26.17 k8s-06 10.57.26.33 k8s-07 EOF host名需要和 hostname /etc/hostname 一致 3:安全相关 setenforce 0 sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config systemctl disable firewalld.service systemctl stop firewalld.service systemctl disable iptables.</description>
    </item>
    
    <item>
      <title>Hugo&#43;github搭建个人网站</title>
      <link>https://pj1987111.github.io/posts/hugo/hugo&#43;github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</link>
      <pubDate>Sat, 22 Jun 2019 22:13:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/hugo/hugo&#43;github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</guid>
      <description>一 安装Hugo 在github上找到相应版本github地址，本人是mac，就下载了hugo_0.55.6_macOS-64bit.tar.gz。下载解压后，将hugo添加进环境变量中。
二 在Hugo中写文章 在硬盘中选取合适的存储路径，然后命令行中使用如下指令生成网页本地文档：
hugo new site personal-site  由此可得到如下文件目录：
personal-site ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── static └── themes 常用目录用处如下
   子目录名称 功能     archetypes 新文章默认模板   config.toml Hugo配置文档   content 存放所有Markdown格式的文章   layouts 存放自定义的view，可为空   static 存放图像、CNAME、css、js等资源，发布后该目录下所有资源将处于网页根目录   themes 存放下载的主题    使用下面的命令生成新的文章草稿：
hugo new posts/first-post.md 在content目录中会自动以archetypes/default.md为模板在content/posts目录下生成一篇名为first-post.md的文章草稿：
--- title: &amp;quot;First Post&amp;quot; date: 2017-12-27T23:15:53-05:00 draft: true ---  我们可以加一个标题在下面并去掉标记为草稿的这一行：draft: true</description>
    </item>
    
    <item>
      <title>Kubeadm离线安装单master节点k8s</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:11 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</guid>
      <description>1 目标 1.在所有节点上安装docker和kubeadm
2.部署kubernetes master
3.部署容器网络插件
4.部署kubernetes worker
5.部署dashboard可视化插件
6.部署容器存储插件
2 准备(所有安装机器均需执行) 2.1:/etc/hosts中添加ip和host的映射 2.2:master和node做互信 2.3:关闭防火墙和selinux 1) 关闭selinux
sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config  2) 关闭防火墙
systemctl disable firewalld.service systemctl stop firewalld.service  3) 关闭iptables
systemctl disable iptables.service systemctl stop iptables.service  2.4:安装docker 将离线包解压，进docker目录，运行sudo yum install * 安装好后再执行
systemctl start docker systemctl enable docker  将docker服务启动
2.5:私仓搭建 这里选择了docker的registry2来建立私有仓库。
 下载镜像仓库
docker pull registry:2  启动镜像仓库
docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 --restart=always --name registry registry:2  配置阿里云的Docker加速器，加快pull registry镜像</description>
    </item>
    
    <item>
      <title>K8s安装离线依赖库与镜像制作</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</guid>
      <description>一：docker环境下载 1.1 yum仓库添加 vim /etc/yum.repos.d/docker-ce.repo 添加以下内容： [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg  推荐添加阿里云的源，速度非常快。
1.2 docker环境本地下载 yum install --downloadonly --downloaddir=. docker-ce-18.09.4-3.el7  该命令会把docker以及依赖全下载到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
二：kubeadm环境下载 2.1 yum仓库添加 vim /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg  2.2 kubeadm环境本地下载 yum install --downloadonly --downloaddir=. kubeadm  该命令会把kubeadm以及依赖全下载包括kubelet,kubectl到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
三：k8s依赖镜像下载 kubeadm安装时会访问特定的tag，若不存在就会走外网下载，所以kubeadm部署前必须将所需的k8s环境以镜像方式下载好，并导入docker中
3.1 阿里云海外镜像下载配置 由于各种原因，k8s的依赖kube-proxy，kube-apiserver，kube-controller-manager，kube-scheduler，coredns，etcd和pause都难于下载，本人通过阿里云海外镜像下载方式，分享一下。 1) 将github上建立一个项目，github的配置不再赘述。 2) 将所有依赖建立目录，并在每个目录下建立Dockerfile，并输入FROM命令，每个组件的命令如下：
FROM k8s.gcr.io/kube-apiserver:v1.14.2 FROM k8s.gcr.io/kube-proxy:v1.14.2 FROM k8s.gcr.io/kube-controller-manager:v1.14.2 FROM k8s.gcr.io/kube-scheduler:v1.14.2 FROM k8s.</description>
    </item>
    
    <item>
      <title>harbor仓库搭建手册</title>
      <link>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</link>
      <pubDate>Fri, 21 Jun 2019 23:21:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</guid>
      <description>参考 http://wiki.tongdun.me/pages/viewpage.action?pageId=20153039
一 harbor介绍 二 环境准备 centos 7.6.1810 docker 18.09.4 docker-compose 1.21.2
2.1 安装docker 2.2 安装docker-compose 下载并解压后，将docker-compose赋权限并放置环境变量下。
三 搭建harbor 3.1 下载离线包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz  下载并解压
3.2 修改harbor配置(http) harbor ui端口默认是80端口，会和大多数应用冲突，所以需要更改，要改两个地方。第一个地方是docker-compose.yml文件，将ports 80:80改成 xxx:80即可，如下图所示： 第二个地方是harbor.yml文件，将http:port: 80更改成需要的，并将hostname改成ip或域名，这个hostname需要与pull和push的前缀一致。如图所示 3.3 生成相关配置并安装 运行 ./prepare以及./install.sh
3.4 启动 docker-compose up -d  3.5 启动后相关容器 正常启动后会有9个容器，如下图所示，其中nginx-photon:v1.8.0容器可以看到端口映射4001-&amp;gt;80 四、客户端docker配置 vi /etc/docker/daemon.json 将&amp;quot;10.57.30.22:4001&amp;quot;添加insecure-registries中 systemctl daemon-reload systemctl restart docker  五、访问 Web UI 并测试 5.1、主页 访问主页http://10.57.30.22:4001 即可看到登陆页面，输入密码后进入，默认账户是admin，密码在harbor.yml中的harbor_admin_password字段。 进入后如下显示： 5.2 创建项目 Harbor 有一个项目的概念，项目名可以理解为 Docker Hub 的用户名，其下可以后很多 images，Harbor 的项目必须登录后方可 push，公有项目和私有项目的区别是对其他用户是否可见 创建项目后，才可以push和pull。</description>
    </item>
    
    <item>
      <title>k8s部署jenkins以及基本使用</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 21 Jun 2019 16:52:58 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>一：Jenkins+Kubernetes方案简介 1.1、传统Jenkins部署的问题 Jenkins 是开源的一套持续集成框架，可以进行大规模的编译、测试和发布的工作，给软件开发团队带来极大的便利性。 Jenkins 的持续集成环境可以是集群化的，主要的运行模式为master-slave模式。 Jenkins 的master为Jenkins系统的控制节点，slave节点负责具体的项目编译测试等工作。 由于大公司里面需要进行编译的工程或者对象非常庞大，因此需要大量的物理节点作为slave,而且这些环节相对固定，可能很难适应其他项目的编译测试，一旦salve节点遭到破坏，需要人为的进行修复甚至重建，非常麻烦。 2.2、与Kubernetes-plugin的结合 Docker的问世，为我们提供了解决方案，使用docker作为jenkins slave节点可以解决slave节点遭到破坏后遇到的问题， 而且大量的工程不是每时每刻同时运行的因此可以在需要时吧docker拉起来进行编译测试，这样就节约了大量的物理节点， 解决了上述问题。 然而使用docker 同样需要集群，因此要用到集群管理工具， swarm 或者kubernetes。Swarm 一般用在小集群上，而且swarm和docker本身的借口完全一致， 因此这里就简单介绍单点docker节点作为slave。 Kubernetes作为大的docker 集群管理工具，当jenkins的工程数量非常大的时候可以用kubernetes, kubernetes的运维相对比docker 和swarm的门槛要高一点。 Jenkins master也可以使用docker来运行或者使用kubernetes来提供高可用的jenkins master。 Kubernetes 主要是用来进行docker 调度运行的，同样因为这一点通过kubernetes插件的配置，不需要管slave节点是否在线，因为kubernetes帮你负责创建和连接。这样就可以做到没有任务的时候没有任务slave节点在线，做到完全的按需启动slave和调度slave。
二、在K8S上构建Jenkins Server 2.1 创建PVC存储 首先需要对jenkins-server创建一个Kubernetes持久化存储卷。为了让Jenkins Server可以具有Fail Over的能力，建议将Jenkins的数据存储到远程存储或者分布式存储上。本文使用分布式存储ceph来模拟存储效果，分布式存储的部署请参考我的《rook+ceph部署分布式文件系统》，指定rook-ceph-block并分配200G空间。
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cicdpvc namespace: cicd labels: app: cicd spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 200Gi  2.2 创建jenkins-server 创建jenkins-server的yaml如下所示
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: jenkins2 namespace: cicd spec: template: metadata: labels: app: jenkins2 spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins2 containers: - name: jenkins image: 10.</description>
    </item>
    
  </channel>
</rss>