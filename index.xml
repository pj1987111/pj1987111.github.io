<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ZHY ZONE</title>
    <link>https://pj1987111.github.io/</link>
    <description>Recent content on ZHY ZONE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Jul 2019 22:12:05 +0800</lastBuildDate>
    
	<atom:link href="https://pj1987111.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Keepalived&#43;HAproxy实现高可用负载均衡</title>
      <link>https://pj1987111.github.io/posts/linux/keepalived&#43;haproxy%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Fri, 05 Jul 2019 22:12:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/linux/keepalived&#43;haproxy%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>1 介绍 Keepalived是一个类似于layer3, 4 &amp;amp; 5交换机制的软件，也就是我们平时说的第3层、第4层和第5层交换。Keepalived的作用是检测web服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的web服务器从系统中剔除，当web服务器工作正常后Keepalived自动将web服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的web服务器
HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案。HAProxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。HAProxy运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。
这里我利用HAproxy对nginx服务器进行负载，然后用Keepalived对HAproxy进行监控：
（主）服务器A：dataocean-04 （从）服务器B：dataocean-03 （VIP)：10.10.51.230
Keepalived监控A、B上的HAproxy，利用Keepalived的VIP漂移技术，若A、B上的HAprox都工作正常，则VIP与优先级别高的服务器（主服务器）绑定，当主服务器当掉时，则与从服务器绑定，而VIP则是暴露给外部访问的ip；HAproxy利用Keepalived生产的VIP对应用所在的nginx进行高可用保障。
部署图如下图所示。 2 HAProxy安装 2.1 编译安装 tar zxvf haproxy-1.7.8.tar.gz cd haproxy-1.7.8 make TARGET=linux2628 PREFIX=/usr/local/haproxy make install PREFIX=/usr/local/haproxy  2.2 配置 2.2.1 编写配置文件 默认安装目录下没有配置文件，只有”doc”，“sbin”，“share”三个目录，可手工创建目录及配置文件； 配置文件中使用的应用程序ip是10.57.26.5，实际使用时需要修改。
mkdir -p /usr/local/haproxy/etc cat&amp;gt;&amp;gt;/usr/local/haproxy/etc/haproxy.cfg&amp;lt;&amp;lt;EOF #全局配置, 用于设定义全局参数, 属于进程级的配置, 通常与操作系统配置有关. global #定义全局日志, 配置在本地, 通过local0 输出, 默认是info级别，可配置两条 log 127.0.0.1 local0 warning #定义日志级别【error warning info debug】 #log 127.0.0.1 local1 info #运行路径 chroot /usr/local/haproxy #PID 文件存放路径 pidfile /var/run/haproxy.pid #设置每haproxy进程的最大并发连接数, 其等同于命令行选项“-n”; “ulimit -n”自动计算的结果参照此参数设定.</description>
    </item>
    
    <item>
      <title>sed命令</title>
      <link>https://pj1987111.github.io/posts/tools/sed/</link>
      <pubDate>Tue, 02 Jul 2019 19:26:07 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/tools/sed/</guid>
      <description>1 sed字符串替换  sed替换的基本语法为:
sed &#39;s/原字符串/替换字符串/&#39;  单引号里面,s表示替换,三根斜线中间是替换的样式,特殊字符需要使用反斜线”\”进行转义。
 单引号” ‘ ’”是没有办法用反斜线”\”转义的,这时候只要把命令中的单引号改为双引号就行了,格式如下：
# 要处理的字符包含单引号 sed &amp;quot;s/原字符串包含&#39;/替换字符串包含&#39;/&amp;quot;   命令中的三根斜线分隔符可以换成别的符号,有时候替换目录字符串的时候有较多斜线，这个时候换成其它的分割符是较为方便,只需要紧跟s定义即可。
# 将分隔符换成问号”?”: sed &#39;s?原字符串?替换字符串?&#39;   可以在末尾加g替换每一个匹配的关键字,否则只替换每行的第一个,例如:
# 替换所有匹配关键字 sed &#39;s/原字符串/替换字符串/g&#39;   一些特殊字符的使用 ^ 表示行首 $符号如果在引号中表示行尾，但是在引号外却表示末行(最后一行)
# 注意这里的 &amp;quot; &amp;amp; &amp;quot; 符号，如果没有 “&amp;amp;”，就会直接将匹配到的字符串替换掉 sed &#39;s/^/添加的头部&amp;amp;/g&#39; #在所有行首添加 sed &#39;s/$/&amp;amp;添加的尾部/g&#39; #在所有行末添加 sed &#39;2s/原字符串/替换字符串/g&#39;　#替换第2行 sed &#39;$s/原字符串/替换字符串/g&#39; #替换最后一行 sed &#39;2,5s/原字符串/替换字符串/g&#39; #替换2到5行 sed &#39;2,$s/原字符串/替换字符串/g&#39; #替换2到最后一行   批量替换字符串 c sed -i &amp;quot;s/查找字段/替换字段/g&amp;quot; `grep 查找字段 -rl 路径` sed -i &amp;quot;s/oldstring/newstring/g&amp;quot; `grep oldstring -rl yourdir   sed处理过的输出是直接输出到屏幕上的,使用参数”i”直接在文件中替换。</description>
    </item>
    
    <item>
      <title>kubeedge安装与部署</title>
      <link>https://pj1987111.github.io/posts/kubeedge/kubeedge%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:04 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/kubeedge/kubeedge%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/</guid>
      <description>1 设备定义 cloud node 10.57.26.15 edge node 10.57.26.26
2 环境依赖配置 1:安装k8s 这一步不再赘述，需要注意的是，官方文档里推荐打开apiserver 8080端口，但是6443安全端口也是可以的。 2:配置Go和kubeedge环境(云端和边缘端都需要安装)
#创建kubeedge源码存放跟路径 mkdir -p kubeedge/src #设置GOPATH export GOPATH=/root/kubeedge #下载go lang包 wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz #解压 tar -xvf go1.10.3.linux-amd64.tar.gz #移动到/usr/local目录下 mv go /usr/local #配置GOROOT环境变量 export GOROOT=/usr/local/go #更新PATH环境变量 export PATH=$GOPATH/bin:$GOROOT/bin:$PATH #下载kubeedge源码于GOPATH路径 git clone https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge  3 生成证书 该步骤将生成kubeedge需要的证书，包含rootCA证书和cert/key对。 同样的cert/key对可以同时被cloud和edge端使用。 运行以下命令生成证书。
$GOPATH/src/github.com/kubeedge/kubeedge/build/tools/certgen.sh genCertAndKey edge  cerrt和 key将被生成在/etc/kubeedge/ca 和/etc/kubeedge/certs 下
4 拷贝证书到边缘节点 在边缘节点上，执行以下命令
mkdir -p /etc/kubeedge/ca mkdir -p /etc/kubeedge/certs scp -r root@node01:/etc/kubeedge/ca /etc/kubeedge scp -r root@node01:/etc/kubeedge/certs /etc/kubeedge  5 编译和运行云端 1 修改modules.</description>
    </item>
    
    <item>
      <title>k8s高可用架构</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:02 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84/</guid>
      <description>1 与keepalive+VIP对比 网上大量的人使用KeepAlive+VIP的形式完成高可用，这个方式有两个不好的地方：
其一，受限于使用者的网络，无法适用于SDN网络，比如Aliyun的VPC。
其二，虽然是高可用的，但是流量还是单点的，所有node的的网络I/O都会高度集中于一台机器上（VIP)，一旦集群节点增多，pod增多，单机的网络I/O迟早是网络隐患。
2 两种模式 2.1 堆叠式etcd拓扑 堆叠式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在同一台服务器上，共同被kubeadm所管理。
每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。
每个控制平面节点都创建一个本地etcd，这个本地etcd只与该控制平面节点的kube-apiserver通讯，kube-scheduler和kube-controller-manager也是节点独立的。
介绍一下这种方案的优缺点： 优点：这种控制平面和etcd部署在同一个节点的模式比控制平面和etcd分离部署安装容易并且易于管理副本。 缺点：这种模式存在错误耦合的缺点，当一个节点宕机，etcd和控制平面实例都丢失，只能添加更多的控制平面节点来缓和这个问题。
在高可用集群下需要运行至少3个堆叠控制平面节点。
这是kubeadm高可用的默认实现方案，运行kubeadm init和kubeadm join &amp;ndash;control-plane后本地etcd将被自动创建。 2.2 分离式etcd拓扑 分离式拓扑的意思是存储元数据的etcd与控制屏幕节点安装在不同服务器上。
类似于堆叠式架构，每个控制平面节点都运行一组kube-apiserver,kube-scheduler和kube-controller-manager，其中kube-apiserver通过负载均衡暴露给所有worker节点。不同的地方在于，etcd在不同的host上运行，每个etcd都会和每个控制平面上的kube-apiserver交互。
这种拓扑结构将etcd与控制平面进行了分离。这种方式与堆叠式架构相比可以在etcd的节点丢失以及控制平面的节点丢失上有更高的容错性。
然而，这种拓扑结构在机器数量上对比堆叠式架构需要两倍的数量。需要至少3个etcd节点以及3个控制平面节点。 3 安装与部署 在接下来的介绍文章中会对这两种模式的安装部署方式做详细介绍，请看</description>
    </item>
    
    <item>
      <title>kubeadm安装高可用k8s集群</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 24 Jun 2019 14:05:00 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>1 前置安装 1.1 环境相关 安装docker和kubeadm，在每一台机器上都需要安装。
1:ssh可信 2:hosts cat&amp;gt;&amp;gt;/etc/hosts&amp;lt;&amp;lt;EOF 10.57.26.15 k8s-01 10.57.26.7 k8s-02 10.57.26.8 k8s-03 10.57.26.26 k8s-04 10.57.26.11 k8s-05 10.57.26.17 k8s-06 10.57.26.33 k8s-07 EOF host名需要和 hostname /etc/hostname 一致 3:安全相关 setenforce 0 sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config systemctl disable firewalld.service systemctl stop firewalld.service systemctl disable iptables.service systemctl stop iptables.service 4:源和代理 echo proxy=http://10.57.22.8:3128/ &amp;gt;&amp;gt; /etc/yum.conf cat&amp;gt;/etc/yum.repos.d/docker-ce.repo&amp;lt;&amp;lt;EOF [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg EOF cat&amp;gt;/etc/yum.repos.d/kubernetes.repo&amp;lt;&amp;lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.</description>
    </item>
    
    <item>
      <title>Hugo&#43;github搭建个人网站</title>
      <link>https://pj1987111.github.io/posts/hugo/hugo&#43;github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</link>
      <pubDate>Sat, 22 Jun 2019 22:13:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/hugo/hugo&#43;github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</guid>
      <description>一 安装Hugo 在github上找到相应版本github地址，本人是mac，就下载了hugo_0.55.6_macOS-64bit.tar.gz。下载解压后，将hugo添加进环境变量中。
二 在Hugo中写文章 在硬盘中选取合适的存储路径，然后命令行中使用如下指令生成网页本地文档：
hugo new site personal-site  由此可得到如下文件目录：
personal-site ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── static └── themes 常用目录用处如下
   子目录名称 功能     archetypes 新文章默认模板   config.toml Hugo配置文档   content 存放所有Markdown格式的文章   layouts 存放自定义的view，可为空   static 存放图像、CNAME、css、js等资源，发布后该目录下所有资源将处于网页根目录   themes 存放下载的主题    使用下面的命令生成新的文章草稿：
hugo new posts/first-post.md 在content目录中会自动以archetypes/default.md为模板在content/posts目录下生成一篇名为first-post.md的文章草稿：
--- title: &amp;quot;First Post&amp;quot; date: 2017-12-27T23:15:53-05:00 draft: true ---  我们可以加一个标题在下面并去掉标记为草稿的这一行：draft: true</description>
    </item>
    
    <item>
      <title>Kubeadm离线安装单master节点k8s</title>
      <link>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:11 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8D%95master%E8%8A%82%E7%82%B9k8s/</guid>
      <description>1 目标 1.在所有节点上安装docker和kubeadm
2.部署kubernetes master
3.部署容器网络插件
4.部署kubernetes worker
5.部署dashboard可视化插件
6.部署容器存储插件
2 准备(所有安装机器均需执行) 2.1:/etc/hosts中添加ip和host的映射 2.2:master和node做互信 2.3:关闭防火墙和selinux 1) 关闭selinux
sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config  2) 关闭防火墙
systemctl disable firewalld.service systemctl stop firewalld.service  3) 关闭iptables
systemctl disable iptables.service systemctl stop iptables.service  2.4:安装docker 将离线包解压，进docker目录，运行sudo yum install * 安装好后再执行
systemctl start docker systemctl enable docker  将docker服务启动
2.5:私仓搭建 这里选择了docker的registry2来建立私有仓库。
 下载镜像仓库
docker pull registry:2  启动镜像仓库
docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 --restart=always --name registry registry:2  配置阿里云的Docker加速器，加快pull registry镜像</description>
    </item>
    
    <item>
      <title>K8s安装离线依赖库与镜像制作</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Sat, 22 Jun 2019 15:08:06 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E5%AE%89%E8%A3%85%E7%A6%BB%E7%BA%BF%E4%BE%9D%E8%B5%96%E5%BA%93%E4%B8%8E%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</guid>
      <description>一：docker环境下载 1.1 yum仓库添加 vim /etc/yum.repos.d/docker-ce.repo 添加以下内容： [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg  推荐添加阿里云的源，速度非常快。
1.2 docker环境本地下载 yum install --downloadonly --downloaddir=. docker-ce-18.09.4-3.el7  该命令会把docker以及依赖全下载到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
二：kubeadm环境下载 2.1 yum仓库添加 vim /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg  2.2 kubeadm环境本地下载 yum install --downloadonly --downloaddir=. kubeadm  该命令会把kubeadm以及依赖全下载包括kubelet,kubectl到指定目录。 注：若发现目录下没有下载的rpm包，请到/var/cache/yum/{RepositoryName}/packages/目录中找
三：k8s依赖镜像下载 kubeadm安装时会访问特定的tag，若不存在就会走外网下载，所以kubeadm部署前必须将所需的k8s环境以镜像方式下载好，并导入docker中
3.1 阿里云海外镜像下载配置 由于各种原因，k8s的依赖kube-proxy，kube-apiserver，kube-controller-manager，kube-scheduler，coredns，etcd和pause都难于下载，本人通过阿里云海外镜像下载方式，分享一下。 1) 将github上建立一个项目，github的配置不再赘述。 2) 将所有依赖建立目录，并在每个目录下建立Dockerfile，并输入FROM命令，每个组件的命令如下：
FROM k8s.gcr.io/kube-apiserver:v1.14.2 FROM k8s.gcr.io/kube-proxy:v1.14.2 FROM k8s.gcr.io/kube-controller-manager:v1.14.2 FROM k8s.gcr.io/kube-scheduler:v1.14.2 FROM k8s.</description>
    </item>
    
    <item>
      <title>harbor仓库搭建手册</title>
      <link>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</link>
      <pubDate>Fri, 21 Jun 2019 23:21:05 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/harbor%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/</guid>
      <description>一 harbor介绍 二 环境准备 centos 7.6.1810 docker 18.09.4 docker-compose 1.21.2
2.1 安装docker 2.2 安装docker-compose 下载并解压后，将docker-compose赋权限并放置环境变量下。
三 搭建harbor 3.1 下载离线包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz  下载并解压
3.2 修改harbor配置(http) harbor ui端口默认是80端口，会和大多数应用冲突，所以需要更改，要改两个地方。第一个地方是docker-compose.yml文件，将ports 80:80改成 xxx:80即可，如下图所示： 第二个地方是harbor.yml文件，将http:port: 80更改成需要的，并将hostname改成ip或域名，这个hostname需要与pull和push的前缀一致。如图所示 3.3 生成相关配置并安装 运行 ./prepare以及./install.sh
3.4 启动 docker-compose up -d  3.5 启动后相关容器 正常启动后会有9个容器，如下图所示，其中nginx-photon:v1.8.0容器可以看到端口映射4001-&amp;gt;80 四、客户端docker配置 vi /etc/docker/daemon.json 将&amp;quot;10.57.30.22:4001&amp;quot;添加insecure-registries中 systemctl daemon-reload systemctl restart docker  五、访问 Web UI 并测试 5.1、主页 访问主页http://10.57.30.22:4001 即可看到登陆页面，输入密码后进入，默认账户是admin，密码在harbor.yml中的harbor_admin_password字段。 进入后如下显示： 5.2 创建项目 Harbor 有一个项目的概念，项目名可以理解为 Docker Hub 的用户名，其下可以后很多 images，Harbor 的项目必须登录后方可 push，公有项目和私有项目的区别是对其他用户是否可见 创建项目后，才可以push和pull。</description>
    </item>
    
    <item>
      <title>k8s部署jenkins以及基本使用</title>
      <link>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 21 Jun 2019 16:52:58 +0800</pubDate>
      
      <guid>https://pj1987111.github.io/posts/k8s/k8s%E9%83%A8%E7%BD%B2jenkins%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>一：Jenkins+Kubernetes方案简介 1.1、传统Jenkins部署的问题 Jenkins 是开源的一套持续集成框架，可以进行大规模的编译、测试和发布的工作，给软件开发团队带来极大的便利性。 Jenkins 的持续集成环境可以是集群化的，主要的运行模式为master-slave模式。 Jenkins 的master为Jenkins系统的控制节点，slave节点负责具体的项目编译测试等工作。 由于大公司里面需要进行编译的工程或者对象非常庞大，因此需要大量的物理节点作为slave,而且这些环节相对固定，可能很难适应其他项目的编译测试，一旦salve节点遭到破坏，需要人为的进行修复甚至重建，非常麻烦。 2.2、与Kubernetes-plugin的结合 Docker的问世，为我们提供了解决方案，使用docker作为jenkins slave节点可以解决slave节点遭到破坏后遇到的问题， 而且大量的工程不是每时每刻同时运行的因此可以在需要时吧docker拉起来进行编译测试，这样就节约了大量的物理节点， 解决了上述问题。 然而使用docker 同样需要集群，因此要用到集群管理工具， swarm 或者kubernetes。Swarm 一般用在小集群上，而且swarm和docker本身的借口完全一致， 因此这里就简单介绍单点docker节点作为slave。 Kubernetes作为大的docker 集群管理工具，当jenkins的工程数量非常大的时候可以用kubernetes, kubernetes的运维相对比docker 和swarm的门槛要高一点。 Jenkins master也可以使用docker来运行或者使用kubernetes来提供高可用的jenkins master。 Kubernetes 主要是用来进行docker 调度运行的，同样因为这一点通过kubernetes插件的配置，不需要管slave节点是否在线，因为kubernetes帮你负责创建和连接。这样就可以做到没有任务的时候没有任务slave节点在线，做到完全的按需启动slave和调度slave。
二、在K8S上构建Jenkins Server 2.1 创建PVC存储 首先需要对jenkins-server创建一个Kubernetes持久化存储卷。为了让Jenkins Server可以具有Fail Over的能力，建议将Jenkins的数据存储到远程存储或者分布式存储上。本文使用分布式存储ceph来模拟存储效果，分布式存储的部署请参考我的《rook+ceph部署分布式文件系统》，指定rook-ceph-block并分配200G空间。
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cicdpvc namespace: cicd labels: app: cicd spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 200Gi  2.2 创建jenkins-server 创建jenkins-server的yaml如下所示
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: jenkins2 namespace: cicd spec: template: metadata: labels: app: jenkins2 spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins2 containers: - name: jenkins image: 10.</description>
    </item>
    
  </channel>
</rss>